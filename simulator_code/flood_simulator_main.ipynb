{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9e145fe-2a2e-46af-a9f3-069bcdd9e04e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Trend'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlines\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Line2D\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mTrend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mWavelet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mForecast\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'Trend'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time as tm\n",
    "#from numpy import RankWarning\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "from scipy.stats import rankdata\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import genpareto, norm, poisson, expon, gamma\n",
    "from scipy.special import inv_boxcox\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import LogFormatter \n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "from Trend import *\n",
    "from Wavelet import *\n",
    "from Forecast import *\n",
    "from NS_Cluster import *\n",
    "!pip install dataretrieval\n",
    "import dataretrieval.nwis as nwis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247b8e20-6f23-4960-9b3d-3371aeb21a5d",
   "metadata": {},
   "source": [
    "# Plotting Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa1f3593-5132-46fd-aaef-225d707843e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_annual_max_series(time, time_series, data_type, data_unit, f_100, ann_percentile_95, percentile_50):\n",
    "    \"\"\"\n",
    "    Plots the annual maximum time series and highlights specific statistical thresholds.\n",
    "\n",
    "    Parameters:\n",
    "    - time: The time or year associated with each data point.\n",
    "    - time_series: The values of the time series to plot.\n",
    "    - data_type: The type of data being plotted (e.g., 'Rainfall').\n",
    "    - data_unit: The unit of measurement for the data (e.g., 'mm').\n",
    "    - f_100: The 100-year flood threshold value.\n",
    "    - ann_percentile_95: The 95th percentile value.\n",
    "    - percentile_50: The 50th percentile daily flow value.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(time, time_series, marker='o', linestyle='-', color='b')\n",
    "    plt.title(f'Annual Maximum {data_type} Over Years')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel(f'{data_type} {data_unit}')\n",
    "    \n",
    "    # Highlight statistical thresholds\n",
    "    plt.axhline(y=f_100, color='r', linestyle='--', label=f\"100 Year Flood: {f_100:.0f}\")\n",
    "    plt.axhline(y=ann_percentile_95, color='g', linestyle='--', label=f\"95th Percentile: {ann_percentile_95:.0f}\")\n",
    "    plt.axhline(y=percentile_50, color='black', linestyle='--', label=f\"50th Daily Percentile: {percentile_50:.0f}\")\n",
    "    \n",
    "    # Text annotations\n",
    "    plt.text(min(time), f_100, f\"100 Year Flood: {f_100:.0f}\", horizontalalignment='left', verticalalignment='bottom', color='r')\n",
    "    plt.text(min(time), ann_percentile_95, f\"20 Year Flood: {ann_percentile_95:.0f}\", horizontalalignment='left', verticalalignment='bottom', color='g')\n",
    "    plt.text(min(time), percentile_50, f\"50th Percentile Daily Flow: {percentile_50:.0f}\", horizontalalignment='left', verticalalignment='bottom', color='black')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d9f1807-4792-4e77-953b-5802acd516b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_daily_values(df, var_interest, ann_step, base_signal, std_thres, f_100, percentile_50, data_unit):\n",
    "    \"\"\"\n",
    "    Plots daily values against an annual extracted wavelet signal with statistical thresholds.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the daily data.\n",
    "    - var_interest: String, the column name of interest in 'df'.\n",
    "    - ann_step: DataFrame containing the annual step function data.\n",
    "    - base_signal: Float, the baseline signal value for 'Ann_Signal'.\n",
    "    - std_thres: String or float, standard threshold label for the baseline signal.\n",
    "    - f_100: Float, the 100-year flood threshold value.\n",
    "    - percentile_50: Float, the 50th percentile daily flow value.\n",
    "    - data_unit: String, the unit of measurement for the data (e.g., 'm^3/s').\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    if std_thres != 'mean':\n",
    "        std_thres = 'median' \n",
    "    \n",
    "    # Plot the daily data\n",
    "    plt.plot(df.index, df[var_interest], label='Daily Values', alpha=0.5)  # Adjusted for visibility\n",
    "    \n",
    "    # Step function for extracted signal\n",
    "    plt.step(ann_step.index, ann_step['Ann_Signal'], label='Annual Signal (Step Function)', where='post')\n",
    "    \n",
    "    # Add horizontal lines for median, f_100, and 50th percentile values\n",
    "    plt.axhline(y=base_signal, color='g', linestyle='--', label=f\"Standard Annual Signal ({std_thres}): {base_signal:.0f} {data_unit}\")\n",
    "    plt.axhline(y=f_100, color='r', linestyle='--', label=f\"100 Year Flood: {f_100:.0f} {data_unit}\")\n",
    "    plt.axhline(y=percentile_50, color='black', linestyle='--', label=f\"50th Daily Percentile: {percentile_50:.0f} {data_unit}\")\n",
    "    \n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(f'Discharge ({data_unit})')\n",
    "    plt.title('Daily Values vs Annual Extracted Wavelet Signal')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d560a9b3-e4fa-4956-9e7a-7976810bfbbf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_violin(data, group_by_columns, y_column, title, y_label, percentile_95=None, f_100=None):\n",
    "    \"\"\"\n",
    "    Creates a violin plot for specified data aggregated by given columns.\n",
    "\n",
    "    :param data: DataFrame containing the data to plot.\n",
    "    :param group_by_columns: List of column names to group by.\n",
    "    :param y_column: The name of the column to be plotted on the y-axis.\n",
    "    :param title: The title of the plot.\n",
    "    :param y_label: The label for the y-axis.\n",
    "    :param percentile_95: Optional; the y-value at which to draw a horizontal line for the 95th percentile.\n",
    "    :param f_100: Optional; the y-value at which to draw a horizontal line for the 100-year flood.\n",
    "\n",
    "    \"\"\"\n",
    "    # Group the data and reset the index\n",
    "    grouped_data = data.groupby(group_by_columns)[y_column].mean().reset_index()\n",
    "\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Create the violin plot\n",
    "    ax = sns.violinplot(x=group_by_columns[0], y=y_column, data=grouped_data)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(group_by_columns[0])\n",
    "    plt.ylabel(y_label)\n",
    "\n",
    "    # Optionally add threshold lines\n",
    "    if percentile_95 is not None:\n",
    "        plt.axhline(y=percentile_95, color='r', linestyle='--', label='95% Daily Percentile')\n",
    "    if f_100 is not None:\n",
    "        plt.axhline(y=f_100, color='b', linestyle='--', label='Empirical CDF 100 Year Flood')\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Annotation adjustments\n",
    "    text_y_position = ax.get_ylim()[0] + (ax.get_ylim()[1] - ax.get_ylim()[0]) * 0.05\n",
    "    counts = grouped_data.groupby(group_by_columns[0]).size().reset_index(name='counts')\n",
    "    for i, row in counts.iterrows():\n",
    "        ax.text(i, text_y_position, str(int(row['counts'])), horizontalalignment='center', size='small', color='red', weight='semibold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if percentile_95 is not None or f_100 is not None:\n",
    "        plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def uni_plot_violin(data, group_by_columns, y_column, title, y_label, percentile_95=None, f_100=None):\n",
    "    \"\"\"\n",
    "    Creates a violin plot for specified data aggregated by given columns.\n",
    "\n",
    "    :param data: DataFrame containing the data to plot.\n",
    "    :param group_by_columns: List of column names to group by.\n",
    "    :param y_column: The name of the column to be plotted on the y-axis.\n",
    "    :param title: The title of the plot.\n",
    "    :param y_label: The label for the y-axis.\n",
    "    :param percentile_95: Optional; the y-value at which to draw a horizontal line for the 95th percentile.\n",
    "    :param f_100: Optional; the y-value at which to draw a horizontal line for the 100-year flood.\n",
    "\n",
    "    \"\"\"\n",
    "    # Group the data and reset the index\n",
    "    grouped_data = data.groupby(group_by_columns)[y_column].mean().reset_index()\n",
    "\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Create the violin plot\n",
    "    ax = sns.violinplot(x=group_by_columns[0], y=y_column, data=grouped_data)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(group_by_columns[0])\n",
    "    plt.ylabel(y_label)\n",
    "\n",
    "    # Optionally add threshold lines\n",
    "    if percentile_95 is not None:\n",
    "        plt.axhline(y=percentile_95, color='r', linestyle='--', label='95% Daily Percentile')\n",
    "    if f_100 is not None:\n",
    "        plt.axhline(y=f_100, color='b', linestyle='--', label='Empirical CDF 100 Year Flood')\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Annotation adjustments\n",
    "    text_y_position = ax.get_ylim()[0] + (ax.get_ylim()[1] - ax.get_ylim()[0]) * 0.05\n",
    "    counts = grouped_data.groupby(group_by_columns[0]).size().reset_index(name='counts')\n",
    "    for i, row in counts.iterrows():\n",
    "        ax.text(i, text_y_position, str(int(row['counts'])), horizontalalignment='center', size='small', color='red', weight='semibold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if percentile_95 is not None or f_100 is not None:\n",
    "        plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86d00e15-351e-4658-8776-69351ebe771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_storm_intensities(series, percentile_50, base_signal, f_100):\n",
    "    \"\"\"\n",
    "    Plots storm intensities over the years as a violin plot and the progression of storm intensity by day.\n",
    "\n",
    "    Parameters:\n",
    "    - series: DataFrame containing storm data, including 'year', 'intensity', 'index_storms', and 'storm_day'.\n",
    "    - percentile_50: The 50th percentile intensity value for reference in the plots.\n",
    "    - percentile_95: The 95th percentile intensity value used for y-axis limit calculation.\n",
    "    - f_100: The empirical CDF 100 year flood intensity value for reference in the plots.\n",
    "    \"\"\"\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "    # Calculate desired y-axis limits\n",
    "    y_min = 0\n",
    "    y_max = max(series['Intensity'].max()*1.1, percentile_95, f_100)\n",
    "\n",
    "    # Setting the overall figure size\n",
    "    plt.figure(figsize=(20, 8))\n",
    "\n",
    "    # ----- Plot 1: Violin Plot -----\n",
    "    ax1 = plt.subplot(1, 2, 1)  # Create subplot 1\n",
    "    sns.violinplot(x='year', y='daily_flow', data=series, cut=0)\n",
    "\n",
    "    plt.title('Storm Intensities Over Years')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Intensity')\n",
    "    plt.axhline(y=percentile_50, color='black', linestyle='--', label='50% Daily Percentile')\n",
    "    plt.axhline(y=base_signal, color='green', linestyle='--', label='Signal Threshold')\n",
    "    plt.axhline(y=f_100, color='r', linestyle='--', label='Empirical CDF 100 Year Flood')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "\n",
    "    # Set the same y-axis limits\n",
    "    ax1.set_ylim([y_min, y_max])\n",
    "\n",
    "    # ----- Plot 2: Line Plot -----\n",
    "    ax2 = plt.subplot(1, 2, 2)  # Create subplot 2\n",
    "    grouped = series.groupby(['year', 'storm_index'])\n",
    "    unique_years = series['year'].unique()\n",
    "    colors = plt.cm.coolwarm(np.linspace(0, 1, len(unique_years)))  # Adjust colormap as needed\n",
    "    year_color_map = dict(zip(unique_years, colors))\n",
    "\n",
    "    for (year, index_storms), group in grouped:\n",
    "        plt.plot(group['storm_day'], group['daily_flow'], color=year_color_map[year])\n",
    "\n",
    "    plt.axhline(y=percentile_50, color='black', linestyle='--')\n",
    "    plt.axhline(y=base_signal, color='green', linestyle='--')\n",
    "    plt.axhline(y=f_100, color='r', linestyle='--')\n",
    "    plt.title('Storm Intensity Progression by Day')\n",
    "    plt.xlabel('Storm Day')\n",
    "    plt.ylabel('Intensity')\n",
    "\n",
    "    custom_lines = [Line2D([0], [0], color=year_color_map[year], lw=4) for year in unique_years]\n",
    "    plt.legend(custom_lines, [f'Year {year}' for year in unique_years], loc='best', fontsize='small', title=\"Storm Years\")\n",
    "\n",
    "    # Set the same y-axis limits\n",
    "    ax2.set_ylim([y_min, y_max])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def uni_plot_storm_intensities(series, percentile_50, percentile_95, f_100):\n",
    "    \"\"\"\n",
    "    Plots storm intensities over the years as a violin plot and the progression of storm intensity by day.\n",
    "\n",
    "    Parameters:\n",
    "    - series: DataFrame containing storm data, including 'year', 'intensity', 'index_storms', and 'storm_day'.\n",
    "    - percentile_50: The 50th percentile intensity value for reference in the plots.\n",
    "    - percentile_95: The 95th percentile intensity value used for y-axis limit calculation.\n",
    "    - f_100: The empirical CDF 100 year flood intensity value for reference in the plots.\n",
    "    \"\"\"\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "    # Calculate desired y-axis limits\n",
    "    y_min = 0\n",
    "    y_max = max(series['intensity'].max()*1.1, percentile_95, f_100)\n",
    "\n",
    "    # Setting the overall figure size\n",
    "    plt.figure(figsize=(20, 8))\n",
    "\n",
    "    # ----- Plot 1: Violin Plot -----\n",
    "    ax1 = plt.subplot(1, 2, 1)  # Create subplot 1\n",
    "    sns.violinplot(x='year', y='intensity', data=series, cut=0)\n",
    "\n",
    "    plt.title('Storm Intensities Over Years')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Intensity')\n",
    "    plt.axhline(y=percentile_50, color='black', linestyle='--', label='50% Daily Percentile')\n",
    "    plt.axhline(y=f_100, color='r', linestyle='--', label='Empirical CDF 100 Year Flood')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "\n",
    "    # Set the same y-axis limits\n",
    "    ax1.set_ylim([y_min, y_max])\n",
    "\n",
    "    # ----- Plot 2: Line Plot -----\n",
    "    ax2 = plt.subplot(1, 2, 2)  # Create subplot 2\n",
    "    grouped = series.groupby(['year', 'index_storms'])\n",
    "    unique_years = series['year'].unique()\n",
    "    colors = plt.cm.coolwarm(np.linspace(0, 1, len(unique_years)))  # Adjust colormap as needed\n",
    "    year_color_map = dict(zip(unique_years, colors))\n",
    "\n",
    "    for (year, index_storms), group in grouped:\n",
    "        plt.plot(group['storm_day'], group['intensity'], color=year_color_map[year])\n",
    "\n",
    "    plt.axhline(y=percentile_50, color='black', linestyle='--')\n",
    "    plt.axhline(y=f_100, color='r', linestyle='--')\n",
    "    plt.title('Storm Intensity Progression by Day')\n",
    "    plt.xlabel('Storm Day')\n",
    "    plt.ylabel('Intensity')\n",
    "\n",
    "    custom_lines = [Line2D([0], [0], color=year_color_map[year], lw=4) for year in unique_years]\n",
    "    plt.legend(custom_lines, [f'Year {year}' for year in unique_years], loc='best', fontsize='small', title=\"Storm Years\")\n",
    "\n",
    "    # Set the same y-axis limits\n",
    "    ax2.set_ylim([y_min, y_max])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78328a9b-143b-48af-91e8-0251d2e123e3",
   "metadata": {},
   "source": [
    "## General: Import, Export, Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9526030-dc1a-47e9-86fe-e1cab6378ae8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m run_name = \u001b[33m\"\u001b[39m\u001b[33mERA5_NormVAE_Full_KNN\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;66;03m# Name your file output\u001b[39;00m\n\u001b[32m     15\u001b[39m folder = \u001b[33m'\u001b[39m\u001b[33msignal-extraction/results/GCM\u001b[39m\u001b[33m'\u001b[39m  \u001b[38;5;66;03m# Specify desired output folder\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43msns\u001b[49m.set_theme(style = \u001b[33m'\u001b[39m\u001b[33mwhite\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;66;03m# Set theme for all plotting\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Export options(Note that the code will always export a csv of all simulated storms)\u001b[39;00m\n\u001b[32m     19\u001b[39m save_signal = \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Save the wavelet reconstructed signal in a separate csv with residuals from historical timeseries\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "# Input file information\n",
    "time_var = 'datetime'  # Time variable name in source file for time series data\n",
    "var_interest = 'discharge_cfs'\n",
    "vae_signal = '~/signal-extraction/results/ERA5_norm_vae/ERA5_norm_vae_extractr_results_latent_full.csv'\n",
    "site = '11446500' #Site Number: American river at Fair Oaks\n",
    "parameter_code = '00060' # Discharge (cfs)\n",
    "start_date = \"1904-10-01\" # Beginning of record\n",
    "end_date = \"2024-10-01\"\n",
    "\n",
    "# Naming and formatting\n",
    "data_type = \"Streamflow\" # Variable name in time series\n",
    "data_unit = \"cfs\" # Variable unit in time series\n",
    "data_source = \"ERA5\" # Data source, eg. Gauge/location\n",
    "run_name = \"ERA5_NormVAE_Full_KNN\" # Name your file output\n",
    "folder = 'signal-extraction/results/GCM'  # Specify desired output folder\n",
    "sns.set_theme(style = 'white') # Set theme for all plotting\n",
    "\n",
    "# Export options(Note that the code will always export a csv of all simulated storms)\n",
    "save_signal = False  # Save the wavelet reconstructed signal in a separate csv with residuals from historical timeseries\n",
    "save_forecast_signal = False  # Save the forecasted reconstructed signal in a separate csv\n",
    "save_summ_exceeds = True  # Save summary data on threshold exceedances (needed for validation plots across entire period of interest)\n",
    "record_dists = True  # Save the types of distributions fit during simulation\n",
    "save_params = True # Save the nonstationary parameters used\n",
    "\n",
    "# GENERAL\n",
    "steps = 'default'  # time length for forecasting in years, default 20.\n",
    "n_simulations = 1000 # default 1000\n",
    "\n",
    "# Plotting\n",
    "plot_raw = False\n",
    "plot_LASSO = False\n",
    "plot_transform = False\n",
    "plot_wave = False\n",
    "plot_reconstruct = False\n",
    "plot_innovations = False\n",
    "plot_BAIC = False\n",
    "plot_wave_forecasts = False\n",
    "plot_forecasts = False\n",
    "plot_traj = False\n",
    "plot_RV = False\n",
    "plot_pair = False\n",
    "plot_allsims = False\n",
    "plot_onesim = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d890391-5932-419f-9ac2-5faae264c88f",
   "metadata": {},
   "source": [
    "## Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f35ed5b-f940-4c22-97ea-60c0da83b37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORECAST\n",
    "use_RF = 'default' # choice of using regularization term by random forest variable importance and threshold to use (bool, thres<float>). Default False, 0.\n",
    "ssp_scenario = 'default' # choice of SSP emissions pathway (8.5, 7.0, 4.5, 2.6, 1.9). Default \"4.5\".\n",
    "forecast_model = 'default' # choice of model (\"ARIMA\", \"LSTM\", \"default\"), default \"ARIMA\".\n",
    "\n",
    "# CLUSTERING\n",
    "nonstationary_type = 'default' # Define nonstationary type ('KNN', 'KNN_MLE', 'scaled','stationary'). Default \"KNN\".\n",
    "sampling_type = 'default' # Decide whether to model frequency, intensity, duration jointly ('univariate', 'multivariate'). Default \"multivariate\".\n",
    "KNN_sampling = 'default' # Decide whether to sample from KNN for final multivariate storm distribution. Default True."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a791fe7-332e-4a37-b4ab-1b2cf5431964",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d29a7889-6e3a-4e4f-b4f0-bcfb210c5b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OSCILLATIONS\n",
    "sigtest = 'default' # Type of wavelet sigtest 'red' or 'white', default will plot both but use red for extraction\n",
    "siglvl_wave = 0.95  # choice of significance level for wavelet extraction, default is 0.95.\n",
    "smoothing_frac = 1 # Fraction of data used for LOWESS smoothing. Default 1.\n",
    "\n",
    "# FORECAST\n",
    "AR_I_MA_lags = (3,3,3)  # Set maximum number of lags to consider for ARIMA fitting (AR, I, MA). Default (3,3,3).\n",
    "LSTM_seq_len = 3 # Set forecast sequence length. Default 3.\n",
    "LSTM_epochs = 100 # Set number of training epochs. Default 100.\n",
    "LSTM_units = 40 # Set number of LSTM nodes in layer. Default 40.\n",
    "LSTM_act = 'relu' # Set activitation function. Default 'relu'.\n",
    "\n",
    "# CLUSTERING\n",
    "std_thres = 'default'  # choice of threshold for standard exceedances (\"median\", \"mean\"), default of median.\n",
    "siglvl_KS = 0.05  # choice of significance level for univariate parameterization using KS Test, default is 5%.\n",
    "siglvl_chi_sq = 0.05  # choice of significance level for univariate parameterization using Chi Squared, default is 5%.\n",
    "signal_dist = 'Expon' # Set dist to extract intensities ('Expon', 'Gamma' or 'GPD'). Default is Exponential.\n",
    "intensity_dist = 'Expon' # Set dist to extract intensities ('Expon', 'Gamma' or 'GPD'). Default is Exponential.\n",
    "duration_dist = 'Expon' # Set dist to extract durations ('Expon', 'Gamma', or 'GPD'). Default is Exponential.\n",
    "frequency_dist = 'Poisson' # Set dist to extract frequencies ('Poisson'). Default is Poisson.\n",
    "scale_factor = 1  # Adjust scaling factor if using 'scaled' nonstationarity. Default is 1.\n",
    "dist_fix = True # Decide whether to fix distribution fits. If True dists are fixed to those specified, otherwise defaults on fit based on KS/Chi Squared Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4aacd1a1-8f60-453c-a8dd-e81a027d4ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish defaults\n",
    "# GENERAL\n",
    "if steps == 'default':\n",
    "    steps = 20\n",
    "\n",
    "# FORECAST\n",
    "if use_RF == 'default':\n",
    "    use_RF = False, 0\n",
    "if ssp_scenario == 'default':\n",
    "    ssp_scenario = '4.5'\n",
    "if forecast_model != 'LSTM':\n",
    "    forecast_model = 'ARIMA'\n",
    "\n",
    "# CLUSTERING\n",
    "if signal_dist == 'default':\n",
    "    signal_dist = 'Expon'\n",
    "if intensity_dist == 'default':\n",
    "    intensity_dist = 'Expon'\n",
    "if duration_dist == 'default':\n",
    "    duration_dist = 'Expon'\n",
    "if frequency_dist == 'default':\n",
    "    frequency_dist = 'Poisson'\n",
    "\n",
    "if KNN_sampling == 'default':\n",
    "    KNN_sampling = True\n",
    "if sampling_type == 'univariate':\n",
    "    KNN_sampling = False\n",
    "    if nonstationary_type == 'scaled' or nonstationary_type == 'stationary':\n",
    "        nonstationary_type = 'default'\n",
    "if nonstationary_type == 'default' or nonstationary_type == 'KNN' or nonstationary_type == 'KNN_MLE':\n",
    "    dist_fix = True  # Distributions fixed for KNN-signal sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1eed95-68a1-4ffc-a605-6463c10d2450",
   "metadata": {},
   "source": [
    "## Data load and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69d27434-9fba-4e7f-93c2-c6816adc8435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shifted latent values upward by 0.905124 to ensure positivity.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1313/3586433356.py:24: FutureWarning: 'Y' is deprecated and will be removed in a future version, please use 'YE' instead.\n",
      "  latent_annual_max = signal['latent'].resample('Y').max()\n"
     ]
    }
   ],
   "source": [
    "# Load\n",
    "signal = pd.read_csv(vae_signal)\n",
    "\n",
    "# Ensure datetime index (and sorted)\n",
    "signal['time'] = pd.to_datetime(signal['time'], errors='coerce')\n",
    "signal = signal.set_index('time').sort_index()\n",
    "\n",
    "# Make sure 'latent' is numeric\n",
    "signal['mu'] = pd.to_numeric(signal['mu'], errors='coerce')\n",
    "\n",
    "# --- Shift to ensure positivity ---\n",
    "latent_min = signal['mu'].min()\n",
    "if latent_min <= 0:\n",
    "    eps = 1e-6  # nominal epsilon for numerical stability\n",
    "    shift_value = -latent_min + eps\n",
    "    signal['mu'] = signal['mu'] + shift_value\n",
    "    print(f\"Shifted latent values upward by {shift_value:.6f} to ensure positivity.\")\n",
    "\n",
    "# Signal date window\n",
    "sig_start = signal.index.min()\n",
    "sig_end   = signal.index.max()\n",
    "\n",
    "# Annual maxima (calendar year-end). 'Y' or 'A-DEC' is equivalent to 'YE' here.\n",
    "latent_annual_max = signal['mu'].resample('YE').max()\n",
    "\n",
    "# Years (from the index, not the values)\n",
    "latent_years = signal.index.year\n",
    "latent_time = np.unique(latent_years)\n",
    "\n",
    "# Series of annual maxima values\n",
    "latent_time_series = latent_annual_max.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c612ac1-5903-4301-92a5-8f4aaaccc9a3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Download data \n",
    "df = nwis.get_record(sites = site, service = 'dv', start = start_date, end= end_date, parameterCd = parameter_code)\n",
    "df[var_interest] = df[parameter_code+'_Mean']\n",
    "\n",
    "# Ensure df index is also timezone-naive\n",
    "df.index = df.index.tz_localize(None)\n",
    "\n",
    "# Clip to the SAME date range as signal (inclusive)\n",
    "df = df.loc[sig_start:sig_end]\n",
    "\n",
    "# Resample to annual frequency and take the maximum daily\n",
    "annual_max = df[var_interest].resample('YE').max()\n",
    "\n",
    "# Extract the year from each date\n",
    "years = df.index.year\n",
    "\n",
    "# Find the unique years\n",
    "time = np.unique(years)\n",
    "\n",
    "# Convert the series to a DataFrame\n",
    "annual_max_df = annual_max.to_frame(name='annual_max_streamflow')\n",
    "\n",
    "# Identify annual maxima\n",
    "time_series = annual_max_df['annual_max_streamflow'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99bf5700-238d-4bdb-b897-45e9d5aaa2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save original statistics for dataseries\n",
    "time = np.unique(years)\n",
    "og_data = time_series\n",
    "og_mean = np.mean(time_series)\n",
    "og_std = np.std(time_series)\n",
    "lg_mean = np.mean(np.log(time_series))\n",
    "lg_std = np.std(np.log(time_series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddd6883e-14ab-4f11-bcc5-b9b68ffe4989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 Year Flood: 226227.91\n",
      "20 Year Flood: 83840.00\n"
     ]
    }
   ],
   "source": [
    "# Calculate the 95th percentile of annual maximum\n",
    "ann_percentile_95 = np.percentile(time_series, 95)\n",
    "\n",
    "# Calculate the 95th Percentile of daily data\n",
    "percentile_95 = np.percentile(df[var_interest], 95)\n",
    "\n",
    "# Calculate the 95th Percentile of daily data\n",
    "percentile_50 = np.percentile(df[var_interest], 50)\n",
    "\n",
    "# Calculate 100 year flood\n",
    "f_100 = np.exp(norm.ppf(1-1/100, loc=lg_mean, scale=lg_std))\n",
    "\n",
    "print(f\"100 Year Flood: {f_100:.2f}\")\n",
    "print(f\"20 Year Flood: {ann_percentile_95:.2f}\")\n",
    "\n",
    "if plot_raw:\n",
    "    # Plot ann max time series \n",
    "    plot_annual_max_series(time, time_series, data_type, data_unit, f_100, ann_percentile_95, percentile_50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eea4fe-50b6-49da-8af3-13cd223e73c4",
   "metadata": {},
   "source": [
    "## Wavelet oscillatory extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54a9b3eb-564d-41b9-b8ec-15c4e401a31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed = fit_loess_smoothing(time_series, frac = smoothing_frac)\n",
    "detrended = time_series - smoothed\n",
    "shift, time_series, lambda_boxcox = transform_data(time_series, detrended, data_type, plot=plot_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ab8ec16-1374-4e36-97ba-e2176b11845f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red Noise AR1 Coefficient: 0.9194417193485553\n"
     ]
    }
   ],
   "source": [
    "# wavelet transform\n",
    "warnings.filterwarnings('ignore', 'divide by zero encountered in divide', RuntimeWarning)\n",
    "wlt = wavelet(time_series)\n",
    "Cw = CI(wlt, time_series, siglvl_wave, \"r\")\n",
    "C = CI(wlt, time_series, siglvl_wave, \"w\")\n",
    "\n",
    "# Global Wavelet Spectrum\n",
    "plt_dataset = {\n",
    "    'Time': time,\n",
    "    'Period': wlt['period'],\n",
    "    'Avg_Power': wlt['avg_power'],\n",
    "    'Power': wlt['power'],\n",
    "    'COI': wlt['coi'],\n",
    "    'W_noise': C['sig'],\n",
    "    'R_noise': Cw['sig']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40c327f4-f3fe-44d7-90db-347ded2e4f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_wave:\n",
    "    wavelet_plot(plt_dataset, siglvl_wave, data_source, sigtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67fa8a09-39dc-474a-bf3c-0fd327b736e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red Noise AR1 Coefficient: 0.9120268205084432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    }
   ],
   "source": [
    "# wavelet transform\n",
    "warnings.filterwarnings('ignore', 'divide by zero encountered in divide', RuntimeWarning)\n",
    "wlt = wavelet(latent_time_series)\n",
    "Cw = CI(wlt, latent_time_series, siglvl_wave, \"r\")\n",
    "C = CI(wlt, latent_time_series, siglvl_wave, \"w\")\n",
    "\n",
    "# Global Wavelet Spectrum\n",
    "plt_dataset = {\n",
    "    'Time': latent_time,\n",
    "    'Period': wlt['period'],\n",
    "    'Avg_Power': wlt['avg_power'],\n",
    "    'Power': wlt['power'],\n",
    "    'COI': wlt['coi'],\n",
    "    'W_noise': C['sig'],\n",
    "    'R_noise': Cw['sig']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "437f39d2-399b-4f90-b342-d8ad31a187ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_wave:\n",
    "    wavelet_plot(plt_dataset, siglvl_wave, \"Latent\", sigtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4173643d-27d7-4ff1-bf8c-b70dcb4b91c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red Noise Sig Test Results\n",
      "Significant scales:\n",
      "[ 2.    2.03  2.07  2.11  2.14  2.18  2.22  2.26  2.3   2.34  2.38  2.42\n",
      "  2.46  2.51  2.55  2.59  2.64  2.69  2.73  2.78  2.83  2.88  2.93  2.98\n",
      "  3.03  3.08  3.14  3.19  3.25  3.31  3.36  3.42  3.48  3.54  3.61  3.67\n",
      "  3.73  3.8   3.86  3.93  4.    4.07  4.14  4.21  4.29  4.36  4.44  4.52\n",
      "  4.59  4.68  4.76  4.84  4.92  5.01  5.1   5.19  5.28  5.37  5.46  5.56\n",
      "  5.66  5.76  5.86  5.96  6.06  6.17  6.28  6.39  6.5   6.61  6.73  6.84\n",
      "  6.96  7.09  7.21  7.34  7.46  7.59 10.2  10.37 10.56 10.74 10.93 11.12\n",
      " 11.31 11.51 11.71 11.92 12.13 12.34 12.55 12.77 13.  ]\n"
     ]
    }
   ],
   "source": [
    "if sigtest == 'white':\n",
    "    print(\"White Noise Sig Test Results\")\n",
    "    reconstruct, sig_scales = reconstruct(C, latent_time_series)\n",
    "else:\n",
    "    print(\"Red Noise Sig Test Results\")\n",
    "    reconstruct, sig_scales = reconstruct(Cw, latent_time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f8e9676-4e41-4900-8ffe-e2187b200359",
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_reconstruct:\n",
    "    plot_reconstructed_series(latent_time_series, reconstruct, dates=time)\n",
    "if plot_innovations:\n",
    "    plot_distribution_of_innovations(latent_time_series, reconstruct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ad56ab-f24f-464c-a1e8-a772eb33cd7b",
   "metadata": {},
   "source": [
    "## Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "904d90ee-d27e-487d-bdb1-5739f855a4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with lowest AIC: ARIMA(2, 0, 3) with AIC: -152.39552149746706\n",
      "Model with lowest BIC: ARIMA(1, 0, 2) with BIC: -136.54508488096337\n",
      "Selected model: ARIMA(1, 0, 2) with AIC: -148.8168213622309 and BIC: -136.54508488096337\n",
      "                               SARIMAX Results                                \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                   86\n",
      "Model:                 ARIMA(1, 0, 2)   Log Likelihood                  79.408\n",
      "Date:                Fri, 31 Oct 2025   AIC                           -148.817\n",
      "Time:                        03:39:23   BIC                           -136.545\n",
      "Sample:                             0   HQIC                          -143.878\n",
      "                                 - 86                                         \n",
      "Covariance Type:                  opg                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.4200      0.000    888.311      0.000       0.419       0.421\n",
      "ar.L1         -0.5713      0.127     -4.488      0.000      -0.821      -0.322\n",
      "ma.L1         -0.0189      5.070     -0.004      0.997      -9.956       9.918\n",
      "ma.L2         -0.9801      4.982     -0.197      0.844     -10.745       8.785\n",
      "sigma2         0.0086      0.044      0.198      0.843      -0.077       0.094\n",
      "===================================================================================\n",
      "Ljung-Box (L1) (Q):                   0.17   Jarque-Bera (JB):                 2.10\n",
      "Prob(Q):                              0.68   Prob(JB):                         0.35\n",
      "Heteroskedasticity (H):               1.58   Skew:                             0.06\n",
      "Prob(H) (two-sided):                  0.23   Kurtosis:                         3.76\n",
      "===================================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n"
     ]
    }
   ],
   "source": [
    "# Suppress common warnings\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"Non-invertible starting MA parameters found.*\", category=UserWarning, module='statsmodels.*')\n",
    "warnings.filterwarnings(\"ignore\", message=\"Non-stationary starting autoregressive parameters found.*\", category=UserWarning, module='statsmodels.*')\n",
    "\n",
    "if forecast_model == 'LSTM':\n",
    "    # Use LSTM to forecast the reconstructed aggregate\n",
    "    forecast_LSTM, LSTM_log_mse = fit_LSTM(reconstruct, time, steps, latent_time_series, seq_len = LSTM_seq_len, epochs = LSTM_epochs, units = LSTM_units, act = LSTM_act, plot=plot_wave_forecasts)\n",
    "    fin_forecast = forecast_LSTM.flatten()\n",
    "else:\n",
    "    # Fit an aggregate ARMA for the reconstructed aggregate\n",
    "    sum_ARMA, forecast_params, AIC, BIC = fit_arima_model(reconstruct, max_ar=AR_I_MA_lags[0], max_ma=AR_I_MA_lags[1], max_d=AR_I_MA_lags[2], plot=plot_wave_forecasts)\n",
    "    \n",
    "    # Plot and simulate time series with aggregate ARMA\n",
    "    forecasts_agg = np.empty((n_simulations, 1, steps))\n",
    "    forecasts_agg[:] = np.nan\n",
    "    \n",
    "    model = sum_ARMA\n",
    "    print(model.summary())\n",
    "    \n",
    "    for j in range(n_simulations):       \n",
    "        # Generate simulations and store in forecasts array\n",
    "        forecast = model.simulate(anchor='end', nsimulations=steps)\n",
    "        forecasts_agg[j, 0, :] = forecast\n",
    "\n",
    "    if plot_wave_forecasts:\n",
    "        plot_all_forecasts(reconstruct, forecasts_agg, latent_time_series)\n",
    "    \n",
    "    fin_forecast = forecasts_agg[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "092974a1-0cb2-4d2c-a0e1-865e52ba3ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert arrays to DataFrame\n",
    "reconstruct_df = pd.DataFrame({'Year': latent_time,'Ann_Signal': reconstruct})\n",
    "\n",
    "# Save to CSV\n",
    "if save_signal:\n",
    "    reconstruct_df.to_csv(f\"{folder}/Signal_{run_name}.csv\", index=False)\n",
    "\n",
    "if forecast_model == 'LSTM':\n",
    "    reconstruct_forecast = reconstructed_forecasts(reconstruct, fin_forecast, og_data, data_unit, forecast_model, run_name, folder, plot=plot_forecasts, save=save_forecast_signal)\n",
    "else: \n",
    "    # Plot sims reconsturcted with single ARMA\n",
    "    reconstruct_forecast = reconstructed_forecasts(reconstruct, fin_forecast, og_data, data_unit, forecast_model, run_name, folder, plot=plot_forecasts, save=save_forecast_signal)\n",
    "    if save_forecast_signal:\n",
    "        forecasted_agg_ibc.to_csv(f\"{folder}/All_ARIMA_Signal_Forecast_{run_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dc5295-cef1-47ad-a386-3e80a5b30478",
   "metadata": {},
   "source": [
    "## Sub-annual clustering parameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5876452-81be-43af-be70-726b60327bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting annual signal to a step function over the same period as daily values for plotting\n",
    "ann_step = df.copy()  # Assuming og_daily_data has 'datetime' as its index\n",
    "ann_step['Ann_Signal'] = np.nan  # Initialize the column\n",
    "\n",
    "# Loop over each year and signal in the annual data\n",
    "for year, signal in zip(reconstruct_df['Year'], reconstruct_df['Ann_Signal']):\n",
    "    # Correctly assign the signal to the 'Ann_Signal' column for the corresponding year\n",
    "    ann_step.loc[ann_step.index.year == year, 'Ann_Signal'] = signal\n",
    "\n",
    "# Calculate the median value of the 'Ann_Signal' column in 'ann_step'\n",
    "if std_thres == 'mean':\n",
    "    base_signal = annual_max.mean()\n",
    "else:\n",
    "    base_signal = annual_max.median()\n",
    "\n",
    "signal_sd = ann_step['Ann_Signal'].std()\n",
    "\n",
    "if plot_raw:\n",
    "    # Plot daily values and annual signal\n",
    "    plot_daily_values(df, var_interest, ann_step, base_signal, std_thres, f_100, percentile_50, data_unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf5da376-c63d-4e05-8866-9099359ef496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract historic flood events\n",
    "if sampling_type == 'univariate':\n",
    "    df, result_df, cluster_sizes = index_and_count_clusters(ann_step, var_interest, base_signal, 'Exceeds_Std')\n",
    "else:\n",
    "    result_df, multivar_signal = extract_exceedance_clusters(df, reconstruct_df, var_interest, base_signal)\n",
    "\n",
    "cluster_dict_std, sigma0_std = trajectory_dict_plot(df, plot_traj)\n",
    "\n",
    "if save_summ_exceeds:\n",
    "    result_df.to_csv(f\"{folder}/Standard_Summary_Exceedances_{run_name}.csv\", index=False)\n",
    "\n",
    "if plot_pair:\n",
    "    # Plot original distribution of storm signal, frequence, intensity, and duration\n",
    "    if sampling_type == 'univariate':\n",
    "        FIDS_pairplot(result_df, jitter_col=['Frequency'],columns=['Frequency', 'Intensity','Duration'])\n",
    "    else:\n",
    "        FIDS_pairplot(result_df, jitter_col=['Frequency'],columns=['Signal', 'Frequency', 'Intensity','Duration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf7679ab-490f-495f-a8c6-573633bf186e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KS Test Statistic: 0.37902307066465885, p-value: 1.7493012788803487e-11\n",
      "Fit enforced by user.\n",
      "Chi-Squared Test Statistic: 58.581862369235616, p-value: 8.73330316906379e-11\n",
      "Fit enforced by user.\n",
      "KS Test Statistic: 0.4603279533585195, p-value: 1.2108006373863156e-15\n",
      "Fit enforced by user.\n",
      "KS Test Statistic: 0.4230835383556897, p-value: 3.530626350474815e-13\n",
      "Fit enforced by user.\n",
      "Chi-Squared Test Statistic: 13.220156304352724, p-value: 0.27918003549903314\n",
      "Fit enforced by user.\n"
     ]
    }
   ],
   "source": [
    "# Fit univariate distributions\n",
    "if dist_fix == True:\n",
    "    dist_fix = -1\n",
    "else:\n",
    "    dist_fix = 0\n",
    "\n",
    "if sampling_type == 'univariate':\n",
    "    freq_params, og_frequency_samples, frequency_dist, freq_test = fit_frequency_distribution(result_df.groupby('Year', as_index=False)['Frequency'].mean()['Frequency'], frequency_dist, plot_RV, \"Frequency\", 100, status=dist_fix) #result_df.groupby('Year')['Frequency'].mean().reset_index()['Frequency']\n",
    "    intensity_params, og_intensity_samples, intensity_dist, int_test = fit_intensity_distribution(result_df['Intensity'][result_df['Frequency'] > 0], intensity_dist, plot_RV, \"Intensity\", 100, status=dist_fix)\n",
    "    duration_params, og_duration_samples, duration_dist, dur_test = fit_duration_distribution(result_df['Duration'][result_df['Frequency'] > 0], duration_dist, plot_RV, \"Duration\", 100, status=dist_fix)\n",
    "\n",
    "    dist_params = {\n",
    "        'intensity': (intensity_params, intensity_dist),\n",
    "        'duration': (duration_params, duration_dist),\n",
    "        'frequency': (freq_params, frequency_dist),\n",
    "    }\n",
    "\n",
    "    dists = pd.DataFrame({\n",
    "        'Model_Component': list(dist_params.keys())+['Forecast_Model'],\n",
    "        'Model_Fit': [intensity_dist,duration_dist,frequency_dist, forecast_model if (forecast_model == 'LSTM') else forecast_model + f'({forecast_params})'],\n",
    "        'Model_Evaluation_1': ['KS Test', 'Chi-Squared Test', 'Chi-Squared Test', 'MLSE' if (forecast_model == 'LSTM') else 'AIC'],\n",
    "        'Evaluation_Value_1': [int_test[0], dur_test[0], freq_test[0], LSTM_log_mse if (forecast_model == 'LSTM') else AIC],\n",
    "        'Model_Evaluation_2': ['p_val', 'p_val', 'p_val', 'N/A' if (forecast_model == 'LSTM') else 'BIC'],\n",
    "        'Evaluation_Value_2': [int_test[1], dur_test[1], freq_test[1], LSTM_log_mse if (forecast_model == 'LSTM') else BIC],\n",
    "    })\n",
    "    \n",
    "else:\n",
    "    freq_signal_params, freq_og_signal_samples, freq_signal_dist, freq_sig_test = fit_intensity_distribution(result_df.groupby('Year', as_index=False)['Signal'].mean()['Signal'], signal_dist, plot_RV, \"Signal\", 100, status=dist_fix) #result_df.groupby('Year')['Signal'].mean().reset_index()['Signal'][result_df['Signal'] > 0]\n",
    "    freq_params, og_frequency_samples, frequency_dist, freq_test = fit_frequency_distribution(result_df.groupby('Year', as_index=False)['Frequency'].mean()['Frequency'], frequency_dist, plot_RV, \"Frequency\", 100, status=dist_fix) #result_df.groupby('Year')['Frequency'].mean().reset_index()['Frequency']\n",
    "    signal_params, og_signal_samples, signal_dist, sig_test = fit_intensity_distribution(result_df['Signal'][result_df['Frequency'] > 0], signal_dist, plot_RV, \"Signal\", 100, status=dist_fix) #result_df.groupby('Year')['Signal'].mean().reset_index()['Signal'][result_df['Signal'] > 0]\n",
    "    intensity_params, og_intensity_samples, intensity_dist, int_test = fit_intensity_distribution(result_df['Intensity'][result_df['Frequency'] > 0], intensity_dist, plot_RV, \"Intensity\", 100, status=dist_fix)\n",
    "    duration_params, og_duration_samples, duration_dist, dur_test = fit_duration_distribution(result_df['Duration'][result_df['Frequency'] > 0], duration_dist, plot_RV, \"Duration\", 100, status=dist_fix)\n",
    "\n",
    "    dist_params = {\n",
    "        'signal_freq': (freq_signal_params, freq_signal_dist),\n",
    "        'signal': (signal_params, signal_dist),\n",
    "        'intensity': (intensity_params, intensity_dist),\n",
    "        'duration': (duration_params, duration_dist),\n",
    "        'frequency': (freq_params, frequency_dist),\n",
    "    }\n",
    "\n",
    "    dists = pd.DataFrame({\n",
    "        'Model_Component': list(dist_params.keys())+['Forecast_Model'],\n",
    "        'Model_Fit': [freq_signal_dist,signal_dist,intensity_dist,duration_dist,frequency_dist, forecast_model if (forecast_model == 'LSTM') else forecast_model + f'({forecast_params})'],\n",
    "        'Model_Evaluation_1': ['KS Test', 'KS Test', 'KS Test', 'Chi-Squared Test', 'Chi-Squared Test', 'Log_MSE' if (forecast_model == 'LSTM') else 'AIC'],\n",
    "        'Evaluation_Value_1': [freq_sig_test[0], sig_test[0], int_test[0], dur_test[0], freq_test[0], LSTM_log_mse if (forecast_model == 'LSTM') else AIC],\n",
    "        'Model_Evaluation_2': ['p_val', 'p_val', 'p_val', 'p_val', 'p_val', 'N/A' if (forecast_model == 'LSTM') else 'BIC'],\n",
    "        'Evaluation_Value_2': [freq_sig_test[1], sig_test[1], int_test[1], dur_test[1], freq_test[1], LSTM_log_mse if (forecast_model == 'LSTM') else BIC],\n",
    "    })\n",
    "\n",
    "if record_dists:\n",
    "    dists.to_csv(f\"{folder}/Dists_{run_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a439451f-64fa-4cef-8777-2642fb396e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize forecasting and signal parameterization vector\n",
    "max_time = np.max(time)\n",
    "    \n",
    "# Create a 2D array for the 'year' column\n",
    "years = np.arange(1, steps + 1) + max_time  # Create list of years to forecast for\n",
    "\n",
    "# Repeat 'years' for each simulation (y times)\n",
    "year_column = np.tile(years, n_simulations)\n",
    "\n",
    "# Repeat simulation indices for each year (x times)\n",
    "sim_column = np.repeat(np.arange(n_simulations), steps)\n",
    "\n",
    "if forecast_model == 'LSTM':\n",
    "    signal_column = np.tile(fin_forecast, n_simulations)\n",
    "else:\n",
    "    # Flatten the forecasted_agg_ibc array for the 'signal' column\n",
    "    signal_column = fin_forecast.flatten()\n",
    "    \n",
    "# Create the DataFrame\n",
    "future_signal = pd.DataFrame({\n",
    "    'year': year_column,\n",
    "    'sim': sim_column,\n",
    "    'signal': signal_column\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb0337cb-5866-4949-9fed-823f1abaad39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stationary Signal Parameters Derived Successfully.\n"
     ]
    }
   ],
   "source": [
    "# Specify parameters for univariate fits based on sampling and nonstationarity type\n",
    "if sampling_type != 'univariate':\n",
    "    if nonstationary_type == 'KNN':\n",
    "        future_signal = KNN(future_signal, multivar_signal)\n",
    "        dist_params['signal'] = (signal_params, 'Expon')\n",
    "        dist_params['signal_freq'] = (freq_signal_params, 'Expon')\n",
    "        dist_params['intensity'] = (intensity_params, 'Expon')\n",
    "        dist_params['duration'] = (duration_params, 'Expon')\n",
    "        print(\"Nonstationary Signal Parameters Derived Successfully.\")\n",
    "    elif nonstationary_type == 'KNN_MLE':\n",
    "        future_signal = KNN_MLE(future_signal, multivar_signal)\n",
    "        dist_params['signal'] = (signal_params, 'Expon')\n",
    "        dist_params['signal_freq'] = (freq_signal_params, 'Expon')\n",
    "        dist_params['intensity'] = (intensity_params, 'Expon')\n",
    "        dist_params['duration'] = (duration_params, 'Expon')\n",
    "        print(\"Nonstationary Signal Parameters Derived Successfully.\")\n",
    "    elif nonstationary_type == 'scaled':\n",
    "        min_sig = min(np.min(result_df['Signal']), np.min(future_signal['signal']))\n",
    "        max_sig = max(np.max(result_df['Signal']), np.max(future_signal['signal']))\n",
    "        scaled = (future_signal['signal'] - min_sig) / (max_sig - min_sig) + 0.5\n",
    "        scaled = scaled*scale_factor \n",
    "        if signal_dist != 'Logspline':\n",
    "            future_signal['Scale_Sig'] = scaled*signal_params['scale']\n",
    "        if intensity_dist != 'Logspline':\n",
    "            future_signal['Scale_Int'] = scaled*intensity_params['scale']\n",
    "        if duration_dist != 'Logspline':    \n",
    "            future_signal['Scale_Dur'] = scaled*duration_params['scale']\n",
    "        if frequency_dist != 'Logspline':    \n",
    "            future_signal['Scale_Freq'] = scaled*freq_params['lambda']\n",
    "        print(\"Nonstationary Signal Parameters Derived Successfully.\")\n",
    "    else:\n",
    "        if signal_dist != 'Logspline':\n",
    "            future_signal['Scale_Sig'] = 1*signal_params['scale']\n",
    "        if intensity_dist != 'Logspline':\n",
    "            future_signal['Scale_Int'] = 1*intensity_params['scale']\n",
    "        if duration_dist != 'Logspline':\n",
    "            future_signal['Scale_Dur'] = 1*duration_params['scale']\n",
    "        if frequency_dist != 'Logspline':\n",
    "            future_signal['Scale_Freq'] = 1*freq_params['lambda']\n",
    "        print(\"Stationary Signal Parameters Derived Successfully.\")\n",
    "\n",
    "    if save_params:\n",
    "        future_signal.to_csv(f\"{folder}/Params_{run_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8162685-50f7-4e9e-b413-a22b4ed9c573",
   "metadata": {},
   "source": [
    "## Adapted Neyman-Scott Process storm generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0ce7074-ef64-47ca-8fa2-de21b7b9d350",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation 100 completed out of 1000 in 22.45 seconds.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m     FS_parent \u001b[38;5;241m=\u001b[39m simulate_storm_frequencies(sim, years, fut_sig, result_df, dist_params, KNN_sampling\u001b[38;5;241m=\u001b[39mKNN_sampling, plot_pair\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, plot_RV\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     34\u001b[0m parent \u001b[38;5;241m=\u001b[39m expand_rows_based_on_frequency(FS_parent, sim)\n\u001b[0;32m---> 35\u001b[0m FIDS_parent \u001b[38;5;241m=\u001b[39m \u001b[43msimulate_storm_statistics\u001b[49m\u001b[43m(\u001b[49m\u001b[43msim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfut_sig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdist_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKNN_sampling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mKNN_sampling\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_RV\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m traj_parent \u001b[38;5;241m=\u001b[39m storm_trajectories(FIDS_parent, bootstrap_curve, cluster_dict_std, plot\u001b[38;5;241m=\u001b[39mplot_traj)\n\u001b[1;32m     37\u001b[0m all_data\u001b[38;5;241m.\u001b[39mappend(traj_parent)\n",
      "File \u001b[0;32m~/1_Temporal_Flood_Cluster/NS_Cluster.py:212\u001b[0m, in \u001b[0;36msimulate_storm_statistics\u001b[0;34m(sim, parent, result_df, future_signal, params, KNN_sampling, plot_pair, plot_RV)\u001b[0m\n\u001b[1;32m    209\u001b[0m     FIDS_pairplot(emp_copula, columns\u001b[38;5;241m=\u001b[39munivariate_sample\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m KNN_sampling:\n\u001b[0;32m--> 212\u001b[0m     emp_copula \u001b[38;5;241m=\u001b[39m \u001b[43mKNN_bootstrap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr_signal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memp_copula\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m sample \u001b[38;5;241m=\u001b[39m emp_copula\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    215\u001b[0m parent\u001b[38;5;241m.\u001b[39mloc[index, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIntensity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDuration\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m sample[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIntensity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDuration\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[0;32m~/1_Temporal_Flood_Cluster/NS_Cluster.py:253\u001b[0m, in \u001b[0;36mKNN_bootstrap\u001b[0;34m(predict, summary, sample_size, neighbors)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# Using NearestNeighbors to find k nearest neighbors efficiently\u001b[39;00m\n\u001b[1;32m    252\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(summary) \u001b[38;5;28;01mif\u001b[39;00m neighbors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mlen\u001b[39m(summary)))\n\u001b[0;32m--> 253\u001b[0m nbrs \u001b[38;5;241m=\u001b[39m \u001b[43mNearestNeighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSignal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# Finding the k nearest neighbors for the single prediction value\u001b[39;00m\n\u001b[1;32m    256\u001b[0m distances, indices \u001b[38;5;241m=\u001b[39m nbrs\u001b[38;5;241m.\u001b[39mkneighbors(np\u001b[38;5;241m.\u001b[39marray([[predict]]))\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/sklearn/base.py:1363\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1356\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1359\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1360\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1361\u001b[0m     )\n\u001b[1;32m   1362\u001b[0m ):\n\u001b[0;32m-> 1363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/sklearn/neighbors/_unsupervised.py:179\u001b[0m, in \u001b[0;36mNearestNeighbors.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# NearestNeighbors.metric is not validated yet\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    161\u001b[0m )\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    163\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the nearest neighbors estimator from the training dataset.\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m        The fitted nearest neighbors estimator.\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/sklearn/neighbors/_base.py:697\u001b[0m, in \u001b[0;36mNeighborsBase._fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    688\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminkowski\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    689\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_params_\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     ):\n\u001b[1;32m    691\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    692\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malgorithm=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkd_tree\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not valid for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    693\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminkowski\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m with a weight parameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    694\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtry algorithm=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mball_tree\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    695\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor algorithm=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    696\u001b[0m         )\n\u001b[0;32m--> 697\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tree \u001b[38;5;241m=\u001b[39m \u001b[43mKDTree\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleaf_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meffective_metric_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meffective_metric_params_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32msklearn/neighbors/_binary_tree.pxi:893\u001b[0m, in \u001b[0;36msklearn.neighbors._kd_tree.BinaryTree64.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/sklearn/utils/validation.py:983\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_all_finite should be a bool or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    978\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mensure_all_finite\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    979\u001b[0m     )\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;66;03m# convert to dtype object to conform to Array API to be use `xp.isdtype` later\u001b[39;00m\n\u001b[0;32m--> 983\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    985\u001b[0m estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m    986\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m estimator_name \u001b[38;5;28;01mif\u001b[39;00m estimator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "start_time = tm.time()  # Record the start time before the simulation begins\n",
    "\n",
    "if sampling_type == 'univariate':\n",
    "    all_sims, traj_parent = uni_adapted_NS_Process(n_simulations=n_simulations, \n",
    "                       years=years,\n",
    "                       forecasted_agg_ibc=forecasted_agg_ibc,\n",
    "                       reconstruct_forecast=reconstruct_forecast,\n",
    "                       summary_std=result_df,\n",
    "                       cluster_dict_std=cluster_dict_std,\n",
    "                       folder=folder,\n",
    "                       run_name=run_name,\n",
    "                       base_signal=base_signal,\n",
    "                       percentile_50=percentile_50,\n",
    "                       forecast_model=forecast_model,\n",
    "                       intensity_dist=intensity_dist,\n",
    "                       intensity_params=intensity_params,\n",
    "                       dur_dist=duration_dist,\n",
    "                       duration_params=duration_params,\n",
    "                       KNN_Type=nonstationary_type,\n",
    "                       steps = steps)\n",
    "    end_time = tm.time()  # Record the end time after the simulation ends\n",
    "    elapsed_time = end_time - start_time  # Calculate the elapsed time\n",
    "    print(f\"{n_simulations} Simulations completed in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "else:\n",
    "    all_data = []\n",
    "    for sim in range(n_simulations):\n",
    "        fut_sig = future_signal[future_signal['sim'] == sim].reset_index(drop=True)\n",
    "        if forecast_model == 'LSTM':\n",
    "            FS_parent = simulate_storm_frequencies(sim, years, fut_sig, result_df, dist_params, KNN_sampling=KNN_sampling, plot_pair=False, plot_RV=False)\n",
    "        else:\n",
    "            FS_parent = simulate_storm_frequencies(sim, years, fut_sig, result_df, dist_params, KNN_sampling=KNN_sampling, plot_pair=False, plot_RV=False)\n",
    "        parent = expand_rows_based_on_frequency(FS_parent, sim)\n",
    "        FIDS_parent = simulate_storm_statistics(sim, parent, result_df, fut_sig, dist_params, KNN_sampling=KNN_sampling, plot_pair=False, plot_RV=False)\n",
    "        traj_parent = storm_trajectories(FIDS_parent, bootstrap_curve, cluster_dict_std, plot=plot_traj)\n",
    "        all_data.append(traj_parent)\n",
    "        if (sim+1) % 100 == 0:\n",
    "            end_time = tm.time()  # Record the end time after the simulation ends\n",
    "            elapsed_time = end_time - start_time  # Calculate the elapsed time\n",
    "            print(f\"Simulation {sim+1} completed out of {n_simulations} in {elapsed_time:.2f} seconds.\")\n",
    "        \n",
    "    end_time = tm.time()  # Record the end time after the simulation ends\n",
    "    elapsed_time = end_time - start_time  # Calculate the elapsed time\n",
    "    print(f\"{n_simulations} Simulations Complete. Run time: {elapsed_time:.2f} seconds.\")\n",
    "    \n",
    "    # Concatenate all DataFrames in the list at once\n",
    "    all_sims = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Convert year and sim columns to integers\n",
    "    all_sims = all_sims.astype({'sim': 'int', 'year': 'int'})\n",
    "    all_sims = all_sims[['sim', 'year', 'storm_index', 'unique_storm_id', 'signal', 'Frequency', 'Intensity', 'Duration', 'storm_day', 'daily_flow']]  # Reorder columns\n",
    "    \n",
    "    # Sort the DataFrame by 'unique_storm_id'\n",
    "    all_sims = all_sims.sort_values(by='unique_storm_id')\n",
    "    # Save to CSV\n",
    "    all_sims.to_csv(f\"{folder}/All_Sims_{run_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f05c5d-d3ba-4a3e-a228-1e0d70a132bb",
   "metadata": {},
   "source": [
    "## Summary Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73919d6f-c067-48a5-8005-14b93d498dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_allsims:\n",
    "    if sampling_type == 'univariate':\n",
    "        uni_plot_violin(\n",
    "            data=all_sims, \n",
    "            group_by_columns=['year', 'sim'], \n",
    "            y_column='storm_max_intensity', \n",
    "            title='Maximum Annual Storm Intensities Over Time, All Simulations', \n",
    "            y_label='Max Annual Intensity', \n",
    "            percentile_95=percentile_95,  # Replace with your actual value\n",
    "            f_100=f_100  # Replace with your actual value\n",
    "        )\n",
    "        \n",
    "        # Duration over all sims\n",
    "        uni_plot_violin(\n",
    "            data=all_sims, \n",
    "            group_by_columns=['year', 'sim'], \n",
    "            y_column='storm_duration', \n",
    "            title='Average Annual Storm Duration Over Time, All Simulations', \n",
    "            y_label='Storm Duration (Days)'\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Frequency over all sims\n",
    "        uni_plot_violin(\n",
    "            data=all_sims, \n",
    "            group_by_columns=['year', 'sim'], \n",
    "            y_column='no_storms', \n",
    "            title='Annual Storm Frequencies Over Time, All Simulations', \n",
    "            y_label='Number of Storms'\n",
    "        )\n",
    "    else:\n",
    "        # Intensity over all sims\n",
    "        plot_violin(\n",
    "            data=all_sims, \n",
    "            group_by_columns=['year', 'sim'], \n",
    "            y_column='Intensity', \n",
    "            title='Maximum Annual Storm Intensities Over Time, All Simulations', \n",
    "            y_label='Max Annual Intensity', \n",
    "            percentile_95=percentile_95,  # Replace with your actual value\n",
    "            f_100=f_100  # Replace with your actual value\n",
    "        )\n",
    "        \n",
    "        # Duration over all sims\n",
    "        plot_violin(\n",
    "            data=all_sims, \n",
    "            group_by_columns=['year', 'sim'], \n",
    "            y_column='Duration', \n",
    "            title='Average Annual Storm Duration Over Time, All Simulations', \n",
    "            y_label='Storm Duration (Days)'\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Frequency over all sims\n",
    "        plot_violin(\n",
    "            data=all_sims, \n",
    "            group_by_columns=['year', 'sim'], \n",
    "            y_column='Frequency', \n",
    "            title='Annual Storm Frequencies Over Time, All Simulations', \n",
    "            y_label='Number of Storms'\n",
    "        )\n",
    "    \n",
    "if plot_onesim:\n",
    "    if sampling_type == 'univariate':\n",
    "        uni_plot_storm_intensities(traj_parent, percentile_50, base_signal, f_100)\n",
    "    else:\n",
    "        plot_storm_intensities(traj_parent, percentile_50, base_signal, f_100)\n",
    "\n",
    "print(\"All Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cad46d8-5aff-4c8f-b7e7-2fec54d37f46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
