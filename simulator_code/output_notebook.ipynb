{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9e145fe-2a2e-46af-a9f3-069bcdd9e04e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T22:10:23.214543Z",
     "iopub.status.busy": "2025-11-23T22:10:23.214324Z",
     "iopub.status.idle": "2025-11-23T22:10:27.781372Z",
     "shell.execute_reply": "2025-11-23T22:10:27.780576Z"
    },
    "papermill": {
     "duration": 4.576432,
     "end_time": "2025-11-23T22:10:27.782602",
     "exception": false,
     "start_time": "2025-11-23T22:10:23.206170",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 22:10:24.789178: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-23 22:10:24.799915: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1763935824.817532    1621 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1763935824.824229    1621 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1763935824.837895    1621 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1763935824.837920    1621 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1763935824.837922    1621 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1763935824.837924    1621 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-23 22:10:24.841953: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX512F AVX512_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dataretrieval in /srv/conda/envs/notebook/lib/python3.12/site-packages (1.0.12)\r\n",
      "Requirement already satisfied: requests in /srv/conda/envs/notebook/lib/python3.12/site-packages (from dataretrieval) (2.32.5)\r\n",
      "Requirement already satisfied: pandas==2.* in /srv/conda/envs/notebook/lib/python3.12/site-packages (from dataretrieval) (2.3.3)\r\n",
      "Requirement already satisfied: numpy>=1.26.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pandas==2.*->dataretrieval) (2.3.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pandas==2.*->dataretrieval) (2.9.0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pandas==2.*->dataretrieval) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pandas==2.*->dataretrieval) (2025.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas==2.*->dataretrieval) (1.17.0)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests->dataretrieval) (3.4.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests->dataretrieval) (3.11)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests->dataretrieval) (1.26.20)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests->dataretrieval) (2025.10.5)\r\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time as tm\n",
    "#from numpy import RankWarning\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "from scipy.stats import rankdata\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import genpareto, norm, poisson, expon, gamma\n",
    "from scipy.special import inv_boxcox\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import LogFormatter \n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "from Trend import *\n",
    "from Wavelet import *\n",
    "from Forecast import *\n",
    "from NS_Cluster import *\n",
    "!pip install dataretrieval\n",
    "import dataretrieval.nwis as nwis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247b8e20-6f23-4960-9b3d-3371aeb21a5d",
   "metadata": {
    "papermill": {
     "duration": 0.006285,
     "end_time": "2025-11-23T22:10:27.796367",
     "exception": false,
     "start_time": "2025-11-23T22:10:27.790082",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Plotting Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa1f3593-5132-46fd-aaef-225d707843e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T22:10:27.810477Z",
     "iopub.status.busy": "2025-11-23T22:10:27.809956Z",
     "iopub.status.idle": "2025-11-23T22:10:27.814715Z",
     "shell.execute_reply": "2025-11-23T22:10:27.814139Z"
    },
    "papermill": {
     "duration": 0.012268,
     "end_time": "2025-11-23T22:10:27.815229",
     "exception": false,
     "start_time": "2025-11-23T22:10:27.802961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_annual_max_series(time, time_series, data_type, data_unit, f_100, ann_percentile_95, percentile_50):\n",
    "    \"\"\"\n",
    "    Plots the annual maximum time series and highlights specific statistical thresholds.\n",
    "\n",
    "    Parameters:\n",
    "    - time: The time or year associated with each data point.\n",
    "    - time_series: The values of the time series to plot.\n",
    "    - data_type: The type of data being plotted (e.g., 'Rainfall').\n",
    "    - data_unit: The unit of measurement for the data (e.g., 'mm').\n",
    "    - f_100: The 100-year flood threshold value.\n",
    "    - ann_percentile_95: The 95th percentile value.\n",
    "    - percentile_50: The 50th percentile daily flow value.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(time, time_series, marker='o', linestyle='-', color='b')\n",
    "    plt.title(f'Annual Maximum {data_type} Over Years')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel(f'{data_type} {data_unit}')\n",
    "    \n",
    "    # Highlight statistical thresholds\n",
    "    plt.axhline(y=f_100, color='r', linestyle='--', label=f\"100 Year Flood: {f_100:.0f}\")\n",
    "    plt.axhline(y=ann_percentile_95, color='g', linestyle='--', label=f\"95th Percentile: {ann_percentile_95:.0f}\")\n",
    "    plt.axhline(y=percentile_50, color='black', linestyle='--', label=f\"50th Daily Percentile: {percentile_50:.0f}\")\n",
    "    \n",
    "    # Text annotations\n",
    "    plt.text(min(time), f_100, f\"100 Year Flood: {f_100:.0f}\", horizontalalignment='left', verticalalignment='bottom', color='r')\n",
    "    plt.text(min(time), ann_percentile_95, f\"20 Year Flood: {ann_percentile_95:.0f}\", horizontalalignment='left', verticalalignment='bottom', color='g')\n",
    "    plt.text(min(time), percentile_50, f\"50th Percentile Daily Flow: {percentile_50:.0f}\", horizontalalignment='left', verticalalignment='bottom', color='black')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d9f1807-4792-4e77-953b-5802acd516b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T22:10:27.828812Z",
     "iopub.status.busy": "2025-11-23T22:10:27.828653Z",
     "iopub.status.idle": "2025-11-23T22:10:27.832570Z",
     "shell.execute_reply": "2025-11-23T22:10:27.831939Z"
    },
    "papermill": {
     "duration": 0.011368,
     "end_time": "2025-11-23T22:10:27.833113",
     "exception": false,
     "start_time": "2025-11-23T22:10:27.821745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_daily_values(df, var_interest, ann_step, base_signal, std_thres, f_100, percentile_50, data_unit):\n",
    "    \"\"\"\n",
    "    Plots daily values against an annual extracted wavelet signal with statistical thresholds.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the daily data.\n",
    "    - var_interest: String, the column name of interest in 'df'.\n",
    "    - ann_step: DataFrame containing the annual step function data.\n",
    "    - base_signal: Float, the baseline signal value for 'Ann_Signal'.\n",
    "    - std_thres: String or float, standard threshold label for the baseline signal.\n",
    "    - f_100: Float, the 100-year flood threshold value.\n",
    "    - percentile_50: Float, the 50th percentile daily flow value.\n",
    "    - data_unit: String, the unit of measurement for the data (e.g., 'm^3/s').\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    if std_thres != 'mean':\n",
    "        std_thres = 'median' \n",
    "    \n",
    "    # Plot the daily data\n",
    "    plt.plot(df.index, df[var_interest], label='Daily Values', alpha=0.5)  # Adjusted for visibility\n",
    "    \n",
    "    # Step function for extracted signal\n",
    "    plt.step(ann_step.index, ann_step['Ann_Signal'], label='Annual Signal (Step Function)', where='post')\n",
    "    \n",
    "    # Add horizontal lines for median, f_100, and 50th percentile values\n",
    "    plt.axhline(y=base_signal, color='g', linestyle='--', label=f\"Standard Annual Signal ({std_thres}): {base_signal:.0f} {data_unit}\")\n",
    "    plt.axhline(y=f_100, color='r', linestyle='--', label=f\"100 Year Flood: {f_100:.0f} {data_unit}\")\n",
    "    plt.axhline(y=percentile_50, color='black', linestyle='--', label=f\"50th Daily Percentile: {percentile_50:.0f} {data_unit}\")\n",
    "    \n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(f'Discharge ({data_unit})')\n",
    "    plt.title('Daily Values vs Annual Extracted Wavelet Signal')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d560a9b3-e4fa-4956-9e7a-7976810bfbbf",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-11-23T22:10:27.846270Z",
     "iopub.status.busy": "2025-11-23T22:10:27.846117Z",
     "iopub.status.idle": "2025-11-23T22:10:27.852735Z",
     "shell.execute_reply": "2025-11-23T22:10:27.852154Z"
    },
    "papermill": {
     "duration": 0.013934,
     "end_time": "2025-11-23T22:10:27.853298",
     "exception": false,
     "start_time": "2025-11-23T22:10:27.839364",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_violin(data, group_by_columns, y_column, title, y_label, percentile_95=None, f_100=None):\n",
    "    \"\"\"\n",
    "    Creates a violin plot for specified data aggregated by given columns.\n",
    "\n",
    "    :param data: DataFrame containing the data to plot.\n",
    "    :param group_by_columns: List of column names to group by.\n",
    "    :param y_column: The name of the column to be plotted on the y-axis.\n",
    "    :param title: The title of the plot.\n",
    "    :param y_label: The label for the y-axis.\n",
    "    :param percentile_95: Optional; the y-value at which to draw a horizontal line for the 95th percentile.\n",
    "    :param f_100: Optional; the y-value at which to draw a horizontal line for the 100-year flood.\n",
    "\n",
    "    \"\"\"\n",
    "    # Group the data and reset the index\n",
    "    grouped_data = data.groupby(group_by_columns)[y_column].mean().reset_index()\n",
    "\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Create the violin plot\n",
    "    ax = sns.violinplot(x=group_by_columns[0], y=y_column, data=grouped_data)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(group_by_columns[0])\n",
    "    plt.ylabel(y_label)\n",
    "\n",
    "    # Optionally add threshold lines\n",
    "    if percentile_95 is not None:\n",
    "        plt.axhline(y=percentile_95, color='r', linestyle='--', label='95% Daily Percentile')\n",
    "    if f_100 is not None:\n",
    "        plt.axhline(y=f_100, color='b', linestyle='--', label='Empirical CDF 100 Year Flood')\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Annotation adjustments\n",
    "    text_y_position = ax.get_ylim()[0] + (ax.get_ylim()[1] - ax.get_ylim()[0]) * 0.05\n",
    "    counts = grouped_data.groupby(group_by_columns[0]).size().reset_index(name='counts')\n",
    "    for i, row in counts.iterrows():\n",
    "        ax.text(i, text_y_position, str(int(row['counts'])), horizontalalignment='center', size='small', color='red', weight='semibold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if percentile_95 is not None or f_100 is not None:\n",
    "        plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def uni_plot_violin(data, group_by_columns, y_column, title, y_label, percentile_95=None, f_100=None):\n",
    "    \"\"\"\n",
    "    Creates a violin plot for specified data aggregated by given columns.\n",
    "\n",
    "    :param data: DataFrame containing the data to plot.\n",
    "    :param group_by_columns: List of column names to group by.\n",
    "    :param y_column: The name of the column to be plotted on the y-axis.\n",
    "    :param title: The title of the plot.\n",
    "    :param y_label: The label for the y-axis.\n",
    "    :param percentile_95: Optional; the y-value at which to draw a horizontal line for the 95th percentile.\n",
    "    :param f_100: Optional; the y-value at which to draw a horizontal line for the 100-year flood.\n",
    "\n",
    "    \"\"\"\n",
    "    # Group the data and reset the index\n",
    "    grouped_data = data.groupby(group_by_columns)[y_column].mean().reset_index()\n",
    "\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Create the violin plot\n",
    "    ax = sns.violinplot(x=group_by_columns[0], y=y_column, data=grouped_data)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(group_by_columns[0])\n",
    "    plt.ylabel(y_label)\n",
    "\n",
    "    # Optionally add threshold lines\n",
    "    if percentile_95 is not None:\n",
    "        plt.axhline(y=percentile_95, color='r', linestyle='--', label='95% Daily Percentile')\n",
    "    if f_100 is not None:\n",
    "        plt.axhline(y=f_100, color='b', linestyle='--', label='Empirical CDF 100 Year Flood')\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Annotation adjustments\n",
    "    text_y_position = ax.get_ylim()[0] + (ax.get_ylim()[1] - ax.get_ylim()[0]) * 0.05\n",
    "    counts = grouped_data.groupby(group_by_columns[0]).size().reset_index(name='counts')\n",
    "    for i, row in counts.iterrows():\n",
    "        ax.text(i, text_y_position, str(int(row['counts'])), horizontalalignment='center', size='small', color='red', weight='semibold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if percentile_95 is not None or f_100 is not None:\n",
    "        plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86d00e15-351e-4658-8776-69351ebe771a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T22:10:27.868683Z",
     "iopub.status.busy": "2025-11-23T22:10:27.868524Z",
     "iopub.status.idle": "2025-11-23T22:10:27.877195Z",
     "shell.execute_reply": "2025-11-23T22:10:27.876597Z"
    },
    "papermill": {
     "duration": 0.016472,
     "end_time": "2025-11-23T22:10:27.877891",
     "exception": false,
     "start_time": "2025-11-23T22:10:27.861419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_storm_intensities(series, percentile_50, base_signal, f_100):\n",
    "    \"\"\"\n",
    "    Plots storm intensities over the years as a violin plot and the progression of storm intensity by day.\n",
    "\n",
    "    Parameters:\n",
    "    - series: DataFrame containing storm data, including 'year', 'intensity', 'index_storms', and 'storm_day'.\n",
    "    - percentile_50: The 50th percentile intensity value for reference in the plots.\n",
    "    - percentile_95: The 95th percentile intensity value used for y-axis limit calculation.\n",
    "    - f_100: The empirical CDF 100 year flood intensity value for reference in the plots.\n",
    "    \"\"\"\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "    # Calculate desired y-axis limits\n",
    "    y_min = 0\n",
    "    y_max = max(series['Intensity'].max()*1.1, percentile_95, f_100)\n",
    "\n",
    "    # Setting the overall figure size\n",
    "    plt.figure(figsize=(20, 8))\n",
    "\n",
    "    # ----- Plot 1: Violin Plot -----\n",
    "    ax1 = plt.subplot(1, 2, 1)  # Create subplot 1\n",
    "    sns.violinplot(x='year', y='daily_flow', data=series, cut=0)\n",
    "\n",
    "    plt.title('Storm Intensities Over Years')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Intensity')\n",
    "    plt.axhline(y=percentile_50, color='black', linestyle='--', label='50% Daily Percentile')\n",
    "    plt.axhline(y=base_signal, color='green', linestyle='--', label='Signal Threshold')\n",
    "    plt.axhline(y=f_100, color='r', linestyle='--', label='Empirical CDF 100 Year Flood')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "\n",
    "    # Set the same y-axis limits\n",
    "    ax1.set_ylim([y_min, y_max])\n",
    "\n",
    "    # ----- Plot 2: Line Plot -----\n",
    "    ax2 = plt.subplot(1, 2, 2)  # Create subplot 2\n",
    "    grouped = series.groupby(['year', 'storm_index'])\n",
    "    unique_years = series['year'].unique()\n",
    "    colors = plt.cm.coolwarm(np.linspace(0, 1, len(unique_years)))  # Adjust colormap as needed\n",
    "    year_color_map = dict(zip(unique_years, colors))\n",
    "\n",
    "    for (year, index_storms), group in grouped:\n",
    "        plt.plot(group['storm_day'], group['daily_flow'], color=year_color_map[year])\n",
    "\n",
    "    plt.axhline(y=percentile_50, color='black', linestyle='--')\n",
    "    plt.axhline(y=base_signal, color='green', linestyle='--')\n",
    "    plt.axhline(y=f_100, color='r', linestyle='--')\n",
    "    plt.title('Storm Intensity Progression by Day')\n",
    "    plt.xlabel('Storm Day')\n",
    "    plt.ylabel('Intensity')\n",
    "\n",
    "    custom_lines = [Line2D([0], [0], color=year_color_map[year], lw=4) for year in unique_years]\n",
    "    plt.legend(custom_lines, [f'Year {year}' for year in unique_years], loc='best', fontsize='small', title=\"Storm Years\")\n",
    "\n",
    "    # Set the same y-axis limits\n",
    "    ax2.set_ylim([y_min, y_max])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def uni_plot_storm_intensities(series, percentile_50, percentile_95, f_100):\n",
    "    \"\"\"\n",
    "    Plots storm intensities over the years as a violin plot and the progression of storm intensity by day.\n",
    "\n",
    "    Parameters:\n",
    "    - series: DataFrame containing storm data, including 'year', 'intensity', 'index_storms', and 'storm_day'.\n",
    "    - percentile_50: The 50th percentile intensity value for reference in the plots.\n",
    "    - percentile_95: The 95th percentile intensity value used for y-axis limit calculation.\n",
    "    - f_100: The empirical CDF 100 year flood intensity value for reference in the plots.\n",
    "    \"\"\"\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "    # Calculate desired y-axis limits\n",
    "    y_min = 0\n",
    "    y_max = max(series['intensity'].max()*1.1, percentile_95, f_100)\n",
    "\n",
    "    # Setting the overall figure size\n",
    "    plt.figure(figsize=(20, 8))\n",
    "\n",
    "    # ----- Plot 1: Violin Plot -----\n",
    "    ax1 = plt.subplot(1, 2, 1)  # Create subplot 1\n",
    "    sns.violinplot(x='year', y='intensity', data=series, cut=0)\n",
    "\n",
    "    plt.title('Storm Intensities Over Years')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Intensity')\n",
    "    plt.axhline(y=percentile_50, color='black', linestyle='--', label='50% Daily Percentile')\n",
    "    plt.axhline(y=f_100, color='r', linestyle='--', label='Empirical CDF 100 Year Flood')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "\n",
    "    # Set the same y-axis limits\n",
    "    ax1.set_ylim([y_min, y_max])\n",
    "\n",
    "    # ----- Plot 2: Line Plot -----\n",
    "    ax2 = plt.subplot(1, 2, 2)  # Create subplot 2\n",
    "    grouped = series.groupby(['year', 'index_storms'])\n",
    "    unique_years = series['year'].unique()\n",
    "    colors = plt.cm.coolwarm(np.linspace(0, 1, len(unique_years)))  # Adjust colormap as needed\n",
    "    year_color_map = dict(zip(unique_years, colors))\n",
    "\n",
    "    for (year, index_storms), group in grouped:\n",
    "        plt.plot(group['storm_day'], group['intensity'], color=year_color_map[year])\n",
    "\n",
    "    plt.axhline(y=percentile_50, color='black', linestyle='--')\n",
    "    plt.axhline(y=f_100, color='r', linestyle='--')\n",
    "    plt.title('Storm Intensity Progression by Day')\n",
    "    plt.xlabel('Storm Day')\n",
    "    plt.ylabel('Intensity')\n",
    "\n",
    "    custom_lines = [Line2D([0], [0], color=year_color_map[year], lw=4) for year in unique_years]\n",
    "    plt.legend(custom_lines, [f'Year {year}' for year in unique_years], loc='best', fontsize='small', title=\"Storm Years\")\n",
    "\n",
    "    # Set the same y-axis limits\n",
    "    ax2.set_ylim([y_min, y_max])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78328a9b-143b-48af-91e8-0251d2e123e3",
   "metadata": {
    "papermill": {
     "duration": 0.006231,
     "end_time": "2025-11-23T22:10:27.891218",
     "exception": false,
     "start_time": "2025-11-23T22:10:27.884987",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## General: Import, Export, Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9526030-dc1a-47e9-86fe-e1cab6378ae8",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-11-23T22:10:27.905031Z",
     "iopub.status.busy": "2025-11-23T22:10:27.904850Z",
     "iopub.status.idle": "2025-11-23T22:10:27.909305Z",
     "shell.execute_reply": "2025-11-23T22:10:27.908669Z"
    },
    "papermill": {
     "duration": 0.012213,
     "end_time": "2025-11-23T22:10:27.909793",
     "exception": false,
     "start_time": "2025-11-23T22:10:27.897580",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Input file information\n",
    "time_var = 'datetime'  # Time variable name in source file for time series data\n",
    "var_interest = 'discharge_cfs'\n",
    "vae_signal = '~/signal-extraction/results/ERA5_norm_vae/ERA5_norm_vae_extractr_results_latent_full.csv'\n",
    "site = '11446500' #Site Number: American river at Fair Oaks\n",
    "parameter_code = '00060' # Discharge (cfs)\n",
    "start_date = \"1904-10-01\" # Beginning of record\n",
    "end_date = \"2024-10-01\"\n",
    "\n",
    "# Naming and formatting\n",
    "data_type = \"Streamflow\" # Variable name in time series\n",
    "data_unit = \"cfs\" # Variable unit in time series\n",
    "data_source = \"ERA5\" # Data source, eg. Gauge/location\n",
    "run_name = \"ERA5_NormVAE_Full_KNN\" # Name your file output\n",
    "folder = 'signal-extraction/results/GCM'  # Specify desired output folder\n",
    "sns.set_theme(style = 'white') # Set theme for all plotting\n",
    "\n",
    "# Export options(Note that the code will always export a csv of all simulated storms)\n",
    "save_signal = False  # Save the wavelet reconstructed signal in a separate csv with residuals from historical timeseries\n",
    "save_forecast_signal = False  # Save the forecasted reconstructed signal in a separate csv\n",
    "save_summ_exceeds = True  # Save summary data on threshold exceedances (needed for validation plots across entire period of interest)\n",
    "record_dists = True  # Save the types of distributions fit during simulation\n",
    "save_params = True # Save the nonstationary parameters used\n",
    "\n",
    "# GENERAL\n",
    "steps = 'default'  # time length for forecasting in years, default 20.\n",
    "n_simulations = 1000 # default 1000\n",
    "\n",
    "# Plotting\n",
    "plot_raw = False\n",
    "plot_LASSO = False\n",
    "plot_transform = False\n",
    "plot_wave = False\n",
    "plot_reconstruct = False\n",
    "plot_innovations = False\n",
    "plot_BAIC = False\n",
    "plot_wave_forecasts = False\n",
    "plot_forecasts = False\n",
    "plot_traj = False\n",
    "plot_RV = False\n",
    "plot_pair = False\n",
    "plot_allsims = False\n",
    "plot_onesim = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0acd997b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T22:10:27.922838Z",
     "iopub.status.busy": "2025-11-23T22:10:27.922684Z",
     "iopub.status.idle": "2025-11-23T22:10:27.925173Z",
     "shell.execute_reply": "2025-11-23T22:10:27.924579Z"
    },
    "papermill": {
     "duration": 0.009454,
     "end_time": "2025-11-23T22:10:27.925673",
     "exception": false,
     "start_time": "2025-11-23T22:10:27.916219",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "start_date = \"1940-01-01\"\n",
    "end_date = \"2009-12-31\"\n",
    "data_source = \"ERA5_norm_vae\"\n",
    "run_name = \"ERA5_norm_vae_KFold_ARIMA_default_3\"\n",
    "folder = \"~/signal-extraction/results/ERA5_norm_vae\"\n",
    "steps = 15\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d890391-5932-419f-9ac2-5faae264c88f",
   "metadata": {
    "papermill": {
     "duration": 0.006213,
     "end_time": "2025-11-23T22:10:27.937954",
     "exception": false,
     "start_time": "2025-11-23T22:10:27.931741",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f35ed5b-f940-4c22-97ea-60c0da83b37b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T22:10:27.950799Z",
     "iopub.status.busy": "2025-11-23T22:10:27.950651Z",
     "iopub.status.idle": "2025-11-23T22:10:27.953279Z",
     "shell.execute_reply": "2025-11-23T22:10:27.952667Z"
    },
    "papermill": {
     "duration": 0.009805,
     "end_time": "2025-11-23T22:10:27.953779",
     "exception": false,
     "start_time": "2025-11-23T22:10:27.943974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FORECAST\n",
    "use_RF = 'default' # choice of using regularization term by random forest variable importance and threshold to use (bool, thres<float>). Default False, 0.\n",
    "ssp_scenario = 'default' # choice of SSP emissions pathway (8.5, 7.0, 4.5, 2.6, 1.9). Default \"4.5\".\n",
    "forecast_model = 'default' # choice of model (\"ARIMA\", \"LSTM\", \"default\"), default \"ARIMA\".\n",
    "\n",
    "# CLUSTERING\n",
    "nonstationary_type = 'default' # Define nonstationary type ('KNN', 'KNN_MLE', 'scaled','stationary'). Default \"KNN\".\n",
    "sampling_type = 'default' # Decide whether to model frequency, intensity, duration jointly ('univariate', 'multivariate'). Default \"multivariate\".\n",
    "KNN_sampling = 'default' # Decide whether to sample from KNN for final multivariate storm distribution. Default True."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a791fe7-332e-4a37-b4ab-1b2cf5431964",
   "metadata": {
    "papermill": {
     "duration": 0.005888,
     "end_time": "2025-11-23T22:10:27.965847",
     "exception": false,
     "start_time": "2025-11-23T22:10:27.959959",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d29a7889-6e3a-4e4f-b4f0-bcfb210c5b68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T22:10:27.978871Z",
     "iopub.status.busy": "2025-11-23T22:10:27.978712Z",
     "iopub.status.idle": "2025-11-23T22:10:27.981984Z",
     "shell.execute_reply": "2025-11-23T22:10:27.981382Z"
    },
    "papermill": {
     "duration": 0.010719,
     "end_time": "2025-11-23T22:10:27.982507",
     "exception": false,
     "start_time": "2025-11-23T22:10:27.971788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# OSCILLATIONS\n",
    "sigtest = 'default' # Type of wavelet sigtest 'red' or 'white', default will plot both but use red for extraction\n",
    "siglvl_wave = 0.95  # choice of significance level for wavelet extraction, default is 0.95.\n",
    "smoothing_frac = 1 # Fraction of data used for LOWESS smoothing. Default 1.\n",
    "\n",
    "# FORECAST\n",
    "AR_I_MA_lags = (3,3,3)  # Set maximum number of lags to consider for ARIMA fitting (AR, I, MA). Default (3,3,3).\n",
    "LSTM_seq_len = 3 # Set forecast sequence length. Default 3.\n",
    "LSTM_epochs = 100 # Set number of training epochs. Default 100.\n",
    "LSTM_units = 40 # Set number of LSTM nodes in layer. Default 40.\n",
    "LSTM_act = 'relu' # Set activitation function. Default 'relu'.\n",
    "\n",
    "# CLUSTERING\n",
    "std_thres = 'default'  # choice of threshold for standard exceedances (\"median\", \"mean\"), default of median.\n",
    "siglvl_KS = 0.05  # choice of significance level for univariate parameterization using KS Test, default is 5%.\n",
    "siglvl_chi_sq = 0.05  # choice of significance level for univariate parameterization using Chi Squared, default is 5%.\n",
    "signal_dist = 'Expon' # Set dist to extract intensities ('Expon', 'Gamma' or 'GPD'). Default is Exponential.\n",
    "intensity_dist = 'Expon' # Set dist to extract intensities ('Expon', 'Gamma' or 'GPD'). Default is Exponential.\n",
    "duration_dist = 'Expon' # Set dist to extract durations ('Expon', 'Gamma', or 'GPD'). Default is Exponential.\n",
    "frequency_dist = 'Poisson' # Set dist to extract frequencies ('Poisson'). Default is Poisson.\n",
    "scale_factor = 1  # Adjust scaling factor if using 'scaled' nonstationarity. Default is 1.\n",
    "dist_fix = True # Decide whether to fix distribution fits. If True dists are fixed to those specified, otherwise defaults on fit based on KS/Chi Squared Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4aacd1a1-8f60-453c-a8dd-e81a027d4ea8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T22:10:27.996729Z",
     "iopub.status.busy": "2025-11-23T22:10:27.996571Z",
     "iopub.status.idle": "2025-11-23T22:10:27.999951Z",
     "shell.execute_reply": "2025-11-23T22:10:27.999352Z"
    },
    "papermill": {
     "duration": 0.01111,
     "end_time": "2025-11-23T22:10:28.000544",
     "exception": false,
     "start_time": "2025-11-23T22:10:27.989434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Establish defaults\n",
    "# GENERAL\n",
    "if steps == 'default':\n",
    "    steps = 20\n",
    "\n",
    "# FORECAST\n",
    "if use_RF == 'default':\n",
    "    use_RF = False, 0\n",
    "if ssp_scenario == 'default':\n",
    "    ssp_scenario = '4.5'\n",
    "if forecast_model != 'LSTM':\n",
    "    forecast_model = 'ARIMA'\n",
    "\n",
    "# CLUSTERING\n",
    "if signal_dist == 'default':\n",
    "    signal_dist = 'Expon'\n",
    "if intensity_dist == 'default':\n",
    "    intensity_dist = 'Expon'\n",
    "if duration_dist == 'default':\n",
    "    duration_dist = 'Expon'\n",
    "if frequency_dist == 'default':\n",
    "    frequency_dist = 'Poisson'\n",
    "\n",
    "if KNN_sampling == 'default':\n",
    "    KNN_sampling = True\n",
    "if sampling_type == 'univariate':\n",
    "    KNN_sampling = False\n",
    "    if nonstationary_type == 'scaled' or nonstationary_type == 'stationary':\n",
    "        nonstationary_type = 'default'\n",
    "if nonstationary_type == 'default' or nonstationary_type == 'KNN' or nonstationary_type == 'KNN_MLE':\n",
    "    dist_fix = True  # Distributions fixed for KNN-signal sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1eed95-68a1-4ffc-a605-6463c10d2450",
   "metadata": {
    "papermill": {
     "duration": 0.0065,
     "end_time": "2025-11-23T22:10:28.014153",
     "exception": false,
     "start_time": "2025-11-23T22:10:28.007653",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data load and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69d27434-9fba-4e7f-93c2-c6816adc8435",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T22:10:28.028084Z",
     "iopub.status.busy": "2025-11-23T22:10:28.027900Z",
     "iopub.status.idle": "2025-11-23T22:10:28.042801Z",
     "shell.execute_reply": "2025-11-23T22:10:28.042177Z"
    },
    "papermill": {
     "duration": 0.022415,
     "end_time": "2025-11-23T22:10:28.043359",
     "exception": false,
     "start_time": "2025-11-23T22:10:28.020944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shifted latent values upward by 0.003346 to ensure positivity.\n"
     ]
    }
   ],
   "source": [
    "# Load\n",
    "signal = pd.read_csv(vae_signal)\n",
    "\n",
    "# Ensure datetime index (and sorted)\n",
    "signal['time'] = pd.to_datetime(signal['time'], errors='coerce')\n",
    "signal = signal.set_index('time').sort_index()\n",
    "\n",
    "# Make sure 'latent' is numeric\n",
    "signal['mu'] = pd.to_numeric(signal['mu'], errors='coerce')\n",
    "\n",
    "# --- Shift to ensure positivity ---\n",
    "latent_min = signal['mu'].min()\n",
    "if latent_min <= 0:\n",
    "    eps = 1e-6  # nominal epsilon for numerical stability\n",
    "    shift_value = -latent_min + eps\n",
    "    signal['mu'] = signal['mu'] + shift_value\n",
    "    print(f\"Shifted latent values upward by {shift_value:.6f} to ensure positivity.\")\n",
    "\n",
    "# Signal date window\n",
    "sig_start = signal.index.min()\n",
    "sig_end   = signal.index.max()\n",
    "\n",
    "# Annual maxima (calendar year-end). 'Y' or 'A-DEC' is equivalent to 'YE' here.\n",
    "latent_annual_max = signal['mu'].resample('YE').max()\n",
    "\n",
    "# Years (from the index, not the values)\n",
    "latent_years = signal.index.year\n",
    "latent_time = np.unique(latent_years)\n",
    "\n",
    "# Series of annual maxima values\n",
    "latent_time_series = latent_annual_max.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c612ac1-5903-4301-92a5-8f4aaaccc9a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T22:10:28.056855Z",
     "iopub.status.busy": "2025-11-23T22:10:28.056701Z",
     "iopub.status.idle": "2025-11-23T22:10:28.792201Z",
     "shell.execute_reply": "2025-11-23T22:10:28.791427Z"
    },
    "papermill": {
     "duration": 0.743341,
     "end_time": "2025-11-23T22:10:28.793288",
     "exception": false,
     "start_time": "2025-11-23T22:10:28.049947",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Download data \n",
    "df = nwis.get_record(sites = site, service = 'dv', start = start_date, end= end_date, parameterCd = parameter_code)\n",
    "df[var_interest] = df[parameter_code+'_Mean']\n",
    "\n",
    "# Ensure df index is also timezone-naive\n",
    "df.index = df.index.tz_localize(None)\n",
    "\n",
    "# Clip to the SAME date range as signal (inclusive)\n",
    "df = df.loc[sig_start:sig_end]\n",
    "\n",
    "# Resample to annual frequency and take the maximum daily\n",
    "annual_max = df[var_interest].resample('YE').max()\n",
    "\n",
    "# Extract the year from each date\n",
    "years = df.index.year\n",
    "\n",
    "# Find the unique years\n",
    "time = np.unique(years)\n",
    "\n",
    "# Convert the series to a DataFrame\n",
    "annual_max_df = annual_max.to_frame(name='annual_max_streamflow')\n",
    "\n",
    "# Identify annual maxima\n",
    "time_series = annual_max_df['annual_max_streamflow'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99bf5700-238d-4bdb-b897-45e9d5aaa2e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T22:10:28.808687Z",
     "iopub.status.busy": "2025-11-23T22:10:28.808499Z",
     "iopub.status.idle": "2025-11-23T22:10:28.811636Z",
     "shell.execute_reply": "2025-11-23T22:10:28.811014Z"
    },
    "papermill": {
     "duration": 0.010884,
     "end_time": "2025-11-23T22:10:28.812166",
     "exception": false,
     "start_time": "2025-11-23T22:10:28.801282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save original statistics for dataseries\n",
    "time = np.unique(years)\n",
    "og_data = time_series\n",
    "og_mean = np.mean(time_series)\n",
    "og_std = np.std(time_series)\n",
    "lg_mean = np.mean(np.log(time_series))\n",
    "lg_std = np.std(np.log(time_series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddd6883e-14ab-4f11-bcc5-b9b68ffe4989",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T22:10:28.825500Z",
     "iopub.status.busy": "2025-11-23T22:10:28.825332Z",
     "iopub.status.idle": "2025-11-23T22:10:28.830026Z",
     "shell.execute_reply": "2025-11-23T22:10:28.829422Z"
    },
    "papermill": {
     "duration": 0.012043,
     "end_time": "2025-11-23T22:10:28.830508",
     "exception": false,
     "start_time": "2025-11-23T22:10:28.818465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 Year Flood: 246866.01\n",
      "20 Year Flood: 96190.00\n"
     ]
    }
   ],
   "source": [
    "# Calculate the 95th percentile of annual maximum\n",
    "ann_percentile_95 = np.percentile(time_series, 95)\n",
    "\n",
    "# Calculate the 95th Percentile of daily data\n",
    "percentile_95 = np.percentile(df[var_interest], 95)\n",
    "\n",
    "# Calculate the 95th Percentile of daily data\n",
    "percentile_50 = np.percentile(df[var_interest], 50)\n",
    "\n",
    "# Calculate 100 year flood\n",
    "f_100 = np.exp(norm.ppf(1-1/100, loc=lg_mean, scale=lg_std))\n",
    "\n",
    "print(f\"100 Year Flood: {f_100:.2f}\")\n",
    "print(f\"20 Year Flood: {ann_percentile_95:.2f}\")\n",
    "\n",
    "if plot_raw:\n",
    "    # Plot ann max time series \n",
    "    plot_annual_max_series(time, time_series, data_type, data_unit, f_100, ann_percentile_95, percentile_50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eea4fe-50b6-49da-8af3-13cd223e73c4",
   "metadata": {
    "papermill": {
     "duration": 0.006346,
     "end_time": "2025-11-23T22:10:28.844159",
     "exception": false,
     "start_time": "2025-11-23T22:10:28.837813",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Wavelet oscillatory extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54a9b3eb-564d-41b9-b8ec-15c4e401a31e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T22:10:28.857967Z",
     "iopub.status.busy": "2025-11-23T22:10:28.857772Z",
     "iopub.status.idle": "2025-11-23T22:10:28.875320Z",
     "shell.execute_reply": "2025-11-23T22:10:28.874715Z"
    },
    "papermill": {
     "duration": 0.025177,
     "end_time": "2025-11-23T22:10:28.875835",
     "exception": false,
     "start_time": "2025-11-23T22:10:28.850658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "smoothed = fit_loess_smoothing(time_series, frac = smoothing_frac)\n",
    "detrended = time_series - smoothed\n",
    "shift, time_series, lambda_boxcox = transform_data(time_series, detrended, data_type, plot=plot_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ab8ec16-1374-4e36-97ba-e2176b11845f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T22:10:28.889805Z",
     "iopub.status.busy": "2025-11-23T22:10:28.889639Z",
     "iopub.status.idle": "2025-11-23T22:10:28.959016Z",
     "shell.execute_reply": "2025-11-23T22:10:28.958502Z"
    },
    "papermill": {
     "duration": 0.081484,
     "end_time": "2025-11-23T22:10:28.964142",
     "exception": false,
     "start_time": "2025-11-23T22:10:28.882658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red Noise AR1 Coefficient: 0.9103360564684414\n"
     ]
    }
   ],
   "source": [
    "# wavelet transform\n",
    "warnings.filterwarnings('ignore', 'divide by zero encountered in divide', RuntimeWarning)\n",
    "wlt = wavelet(time_series)\n",
    "Cw = CI(wlt, time_series, siglvl_wave, \"r\")\n",
    "C = CI(wlt, time_series, siglvl_wave, \"w\")\n",
    "\n",
    "# Global Wavelet Spectrum\n",
    "plt_dataset = {\n",
    "    'Time': time,\n",
    "    'Period': wlt['period'],\n",
    "    'Avg_Power': wlt['avg_power'],\n",
    "    'Power': wlt['power'],\n",
    "    'COI': wlt['coi'],\n",
    "    'W_noise': C['sig'],\n",
    "    'R_noise': Cw['sig']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40c327f4-f3fe-44d7-90db-347ded2e4f5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T22:10:28.983834Z",
     "iopub.status.busy": "2025-11-23T22:10:28.983145Z",
     "iopub.status.idle": "2025-11-23T22:10:28.986810Z",
     "shell.execute_reply": "2025-11-23T22:10:28.985950Z"
    },
    "papermill": {
     "duration": 0.01385,
     "end_time": "2025-11-23T22:10:28.987395",
     "exception": false,
     "start_time": "2025-11-23T22:10:28.973545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if plot_wave:\n",
    "    wavelet_plot(plt_dataset, siglvl_wave, data_source, sigtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67fa8a09-39dc-474a-bf3c-0fd327b736e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T22:10:29.007280Z",
     "iopub.status.busy": "2025-11-23T22:10:29.007044Z",
     "iopub.status.idle": "2025-11-23T22:10:29.060965Z",
     "shell.execute_reply": "2025-11-23T22:10:29.060374Z"
    },
    "papermill": {
     "duration": 0.064613,
     "end_time": "2025-11-23T22:10:29.061492",
     "exception": false,
     "start_time": "2025-11-23T22:10:28.996879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red Noise AR1 Coefficient: 0.9680913302634585\n"
     ]
    }
   ],
   "source": [
    "# wavelet transform\n",
    "warnings.filterwarnings('ignore', 'divide by zero encountered in divide', RuntimeWarning)\n",
    "wlt = wavelet(latent_time_series)\n",
    "Cw = CI(wlt, latent_time_series, siglvl_wave, \"r\")\n",
    "C = CI(wlt, latent_time_series, siglvl_wave, \"w\")\n",
    "\n",
    "# Global Wavelet Spectrum\n",
    "plt_dataset = {\n",
    "    'Time': latent_time,\n",
    "    'Period': wlt['period'],\n",
    "    'Avg_Power': wlt['avg_power'],\n",
    "    'Power': wlt['power'],\n",
    "    'COI': wlt['coi'],\n",
    "    'W_noise': C['sig'],\n",
    "    'R_noise': Cw['sig']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "437f39d2-399b-4f90-b342-d8ad31a187ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T22:10:29.075951Z",
     "iopub.status.busy": "2025-11-23T22:10:29.075679Z",
     "iopub.status.idle": "2025-11-23T22:10:29.077965Z",
     "shell.execute_reply": "2025-11-23T22:10:29.077461Z"
    },
    "papermill": {
     "duration": 0.010048,
     "end_time": "2025-11-23T22:10:29.078639",
     "exception": false,
     "start_time": "2025-11-23T22:10:29.068591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if plot_wave:\n",
    "    wavelet_plot(plt_dataset, siglvl_wave, \"Latent\", sigtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4173643d-27d7-4ff1-bf8c-b70dcb4b91c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T22:10:29.092641Z",
     "iopub.status.busy": "2025-11-23T22:10:29.092492Z",
     "iopub.status.idle": "2025-11-23T22:10:29.101877Z",
     "shell.execute_reply": "2025-11-23T22:10:29.101388Z"
    },
    "papermill": {
     "duration": 0.017159,
     "end_time": "2025-11-23T22:10:29.102477",
     "exception": false,
     "start_time": "2025-11-23T22:10:29.085318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red Noise Sig Test Results\n",
      "Significant scales:\n",
      "[ 2.    2.03  2.07  2.11  2.14  2.18  2.22  2.26  2.3   2.34  2.38  2.42\n",
      "  2.46  2.51  2.55  2.59  2.64  2.69  2.73  2.78  2.83  2.88  2.93  2.98\n",
      "  3.03  3.08  3.14  3.19  3.25  3.31  3.36  3.42  3.48  3.54  3.61  3.67\n",
      "  3.73  3.8   3.86  3.93  4.    4.07  4.14  4.21  4.29  4.36  4.44  4.52\n",
      "  4.59  4.68  4.76  4.84  4.92  5.01  5.1   5.19  5.28  5.37  5.46  5.56\n",
      "  5.66  5.76  5.86  5.96  6.06  6.17  6.28  6.39  6.5   6.61  6.73  6.84\n",
      "  6.96  7.09  7.21  7.34  7.46  7.59  7.73  7.86  8.    8.14  8.28  8.43\n",
      "  8.57  8.72  8.88  9.03  9.19  9.35  9.51  9.68  9.85 10.02 10.2  10.37\n",
      " 10.56 10.74 10.93 11.12 11.31 11.51 11.71 11.92 12.13 12.34 12.55 12.77\n",
      " 13.   13.22 13.45 13.69 13.93 14.17 14.42 14.67 14.93 15.19 15.45]\n"
     ]
    }
   ],
   "source": [
    "if sigtest == 'white':\n",
    "    print(\"White Noise Sig Test Results\")\n",
    "    reconstruct, sig_scales = reconstruct(C, latent_time_series)\n",
    "else:\n",
    "    print(\"Red Noise Sig Test Results\")\n",
    "    reconstruct, sig_scales = reconstruct(Cw, latent_time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f8e9676-4e41-4900-8ffe-e2187b200359",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T22:10:29.116857Z",
     "iopub.status.busy": "2025-11-23T22:10:29.116709Z",
     "iopub.status.idle": "2025-11-23T22:10:29.119051Z",
     "shell.execute_reply": "2025-11-23T22:10:29.118552Z"
    },
    "papermill": {
     "duration": 0.010313,
     "end_time": "2025-11-23T22:10:29.119737",
     "exception": false,
     "start_time": "2025-11-23T22:10:29.109424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if plot_reconstruct:\n",
    "    plot_reconstructed_series(latent_time_series, reconstruct, dates=time)\n",
    "if plot_innovations:\n",
    "    plot_distribution_of_innovations(latent_time_series, reconstruct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ad56ab-f24f-464c-a1e8-a772eb33cd7b",
   "metadata": {
    "papermill": {
     "duration": 0.007612,
     "end_time": "2025-11-23T22:10:29.135145",
     "exception": false,
     "start_time": "2025-11-23T22:10:29.127533",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "904d90ee-d27e-487d-bdb1-5739f855a4b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T22:10:29.149993Z",
     "iopub.status.busy": "2025-11-23T22:10:29.149806Z",
     "iopub.status.idle": "2025-11-23T22:10:33.831245Z",
     "shell.execute_reply": "2025-11-23T22:10:33.830585Z"
    },
    "papermill": {
     "duration": 4.690149,
     "end_time": "2025-11-23T22:10:33.832081",
     "exception": false,
     "start_time": "2025-11-23T22:10:29.141932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with lowest AIC: ARIMA(2, 0, 2) with AIC: -874.851816054266\n",
      "Model with lowest BIC: ARIMA(2, 0, 2) with BIC: -860.125732276745\n",
      "Selected model: ARIMA(2, 0, 2) with AIC: -874.851816054266 and BIC: -860.125732276745\n",
      "                               SARIMAX Results                                \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                   86\n",
      "Model:                 ARIMA(2, 0, 2)   Log Likelihood                 443.426\n",
      "Date:                Sun, 23 Nov 2025   AIC                           -874.852\n",
      "Time:                        22:10:32   BIC                           -860.126\n",
      "Sample:                             0   HQIC                          -868.925\n",
      "                                 - 86                                         \n",
      "Covariance Type:                  opg                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0095   2.17e-05    438.140      0.000       0.009       0.010\n",
      "ar.L1          0.0200      0.280      0.071      0.943      -0.529       0.569\n",
      "ar.L2          0.1535      0.194      0.789      0.430      -0.228       0.535\n",
      "ma.L1         -0.2607      0.218     -1.195      0.232      -0.689       0.167\n",
      "ma.L2         -0.6894      0.219     -3.146      0.002      -1.119      -0.260\n",
      "sigma2      1.885e-06   3.64e-07      5.185      0.000    1.17e-06     2.6e-06\n",
      "===================================================================================\n",
      "Ljung-Box (L1) (Q):                   0.11   Jarque-Bera (JB):                 6.78\n",
      "Prob(Q):                              0.74   Prob(JB):                         0.03\n",
      "Heteroskedasticity (H):               1.71   Skew:                            -0.60\n",
      "Prob(H) (two-sided):                  0.16   Kurtosis:                         3.67\n",
      "===================================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n"
     ]
    }
   ],
   "source": [
    "# Suppress common warnings\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"Non-invertible starting MA parameters found.*\", category=UserWarning, module='statsmodels.*')\n",
    "warnings.filterwarnings(\"ignore\", message=\"Non-stationary starting autoregressive parameters found.*\", category=UserWarning, module='statsmodels.*')\n",
    "\n",
    "if forecast_model == 'LSTM':\n",
    "    # Use LSTM to forecast the reconstructed aggregate\n",
    "    forecast_LSTM, LSTM_log_mse = fit_LSTM(reconstruct, time, steps, latent_time_series, seq_len = LSTM_seq_len, epochs = LSTM_epochs, units = LSTM_units, act = LSTM_act, plot=plot_wave_forecasts)\n",
    "    fin_forecast = forecast_LSTM.flatten()\n",
    "else:\n",
    "    # Fit an aggregate ARMA for the reconstructed aggregate\n",
    "    sum_ARMA, forecast_params, AIC, BIC = fit_arima_model(reconstruct, max_ar=AR_I_MA_lags[0], max_ma=AR_I_MA_lags[1], max_d=AR_I_MA_lags[2], plot=plot_wave_forecasts)\n",
    "    \n",
    "    # Plot and simulate time series with aggregate ARMA\n",
    "    forecasts_agg = np.empty((n_simulations, 1, steps))\n",
    "    forecasts_agg[:] = np.nan\n",
    "    \n",
    "    model = sum_ARMA\n",
    "    print(model.summary())\n",
    "    \n",
    "    for j in range(n_simulations):       \n",
    "        # Generate simulations and store in forecasts array\n",
    "        forecast = model.simulate(anchor='end', nsimulations=steps)\n",
    "        forecasts_agg[j, 0, :] = forecast\n",
    "\n",
    "    if plot_wave_forecasts:\n",
    "        plot_all_forecasts(reconstruct, forecasts_agg, latent_time_series)\n",
    "    \n",
    "    fin_forecast = forecasts_agg[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "092974a1-0cb2-4d2c-a0e1-865e52ba3ead",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T22:10:33.849063Z",
     "iopub.status.busy": "2025-11-23T22:10:33.848849Z",
     "iopub.status.idle": "2025-11-23T22:10:33.853350Z",
     "shell.execute_reply": "2025-11-23T22:10:33.852849Z"
    },
    "papermill": {
     "duration": 0.013298,
     "end_time": "2025-11-23T22:10:33.854028",
     "exception": false,
     "start_time": "2025-11-23T22:10:33.840730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert arrays to DataFrame\n",
    "reconstruct_df = pd.DataFrame({'Year': latent_time,'Ann_Signal': reconstruct})\n",
    "\n",
    "# Save to CSV\n",
    "if save_signal:\n",
    "    reconstruct_df.to_csv(f\"{folder}/Signal_{run_name}.csv\", index=False)\n",
    "\n",
    "if forecast_model == 'LSTM':\n",
    "    reconstruct_forecast = reconstructed_forecasts(reconstruct, fin_forecast, og_data, data_unit, forecast_model, run_name, folder, plot=plot_forecasts, save=save_forecast_signal)\n",
    "else: \n",
    "    # Plot sims reconsturcted with single ARMA\n",
    "    reconstruct_forecast = reconstructed_forecasts(reconstruct, fin_forecast, og_data, data_unit, forecast_model, run_name, folder, plot=plot_forecasts, save=save_forecast_signal)\n",
    "    if save_forecast_signal:\n",
    "        forecasted_agg_ibc.to_csv(f\"{folder}/All_ARIMA_Signal_Forecast_{run_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dc5295-cef1-47ad-a386-3e80a5b30478",
   "metadata": {
    "papermill": {
     "duration": 0.006621,
     "end_time": "2025-11-23T22:10:33.867469",
     "exception": false,
     "start_time": "2025-11-23T22:10:33.860848",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sub-annual clustering parameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5876452-81be-43af-be70-726b60327bda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T22:10:33.881787Z",
     "iopub.status.busy": "2025-11-23T22:10:33.881635Z",
     "iopub.status.idle": "2025-11-23T22:10:33.948000Z",
     "shell.execute_reply": "2025-11-23T22:10:33.947493Z"
    },
    "papermill": {
     "duration": 0.074966,
     "end_time": "2025-11-23T22:10:33.948858",
     "exception": false,
     "start_time": "2025-11-23T22:10:33.873892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Converting annual signal to a step function over the same period as daily values for plotting\n",
    "ann_step = df.copy()  # Assuming og_daily_data has 'datetime' as its index\n",
    "ann_step['Ann_Signal'] = np.nan  # Initialize the column\n",
    "\n",
    "# Loop over each year and signal in the annual data\n",
    "for year, signal in zip(reconstruct_df['Year'], reconstruct_df['Ann_Signal']):\n",
    "    # Correctly assign the signal to the 'Ann_Signal' column for the corresponding year\n",
    "    ann_step.loc[ann_step.index.year == year, 'Ann_Signal'] = signal\n",
    "\n",
    "# Calculate the median value of the 'Ann_Signal' column in 'ann_step'\n",
    "if std_thres == 'mean':\n",
    "    base_signal = annual_max.mean()\n",
    "else:\n",
    "    base_signal = annual_max.median()\n",
    "\n",
    "signal_sd = ann_step['Ann_Signal'].std()\n",
    "\n",
    "if plot_raw:\n",
    "    # Plot daily values and annual signal\n",
    "    plot_daily_values(df, var_interest, ann_step, base_signal, std_thres, f_100, percentile_50, data_unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf5da376-c63d-4e05-8866-9099359ef496",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T22:10:33.968415Z",
     "iopub.status.busy": "2025-11-23T22:10:33.968257Z",
     "iopub.status.idle": "2025-11-23T22:10:34.845312Z",
     "shell.execute_reply": "2025-11-23T22:10:34.844621Z"
    },
    "papermill": {
     "duration": 0.888502,
     "end_time": "2025-11-23T22:10:34.846112",
     "exception": false,
     "start_time": "2025-11-23T22:10:33.957610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract historic flood events\n",
    "if sampling_type == 'univariate':\n",
    "    df, result_df, cluster_sizes = index_and_count_clusters(ann_step, var_interest, base_signal, 'Exceeds_Std')\n",
    "else:\n",
    "    result_df, multivar_signal = extract_exceedance_clusters(df, reconstruct_df, var_interest, base_signal)\n",
    "\n",
    "cluster_dict_std, sigma0_std = trajectory_dict_plot(df, plot_traj)\n",
    "\n",
    "if save_summ_exceeds:\n",
    "    result_df.to_csv(f\"{folder}/Standard_Summary_Exceedances_{run_name}.csv\", index=False)\n",
    "\n",
    "if plot_pair:\n",
    "    # Plot original distribution of storm signal, frequence, intensity, and duration\n",
    "    if sampling_type == 'univariate':\n",
    "        FIDS_pairplot(result_df, jitter_col=['Frequency'],columns=['Frequency', 'Intensity','Duration'])\n",
    "    else:\n",
    "        FIDS_pairplot(result_df, jitter_col=['Frequency'],columns=['Signal', 'Frequency', 'Intensity','Duration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf7679ab-490f-495f-a8c6-573633bf186e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T22:10:34.861656Z",
     "iopub.status.busy": "2025-11-23T22:10:34.861486Z",
     "iopub.status.idle": "2025-11-23T22:10:34.879886Z",
     "shell.execute_reply": "2025-11-23T22:10:34.879354Z"
    },
    "papermill": {
     "duration": 0.027074,
     "end_time": "2025-11-23T22:10:34.880641",
     "exception": false,
     "start_time": "2025-11-23T22:10:34.853567",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KS Test Statistic: 0.484139014015063, p-value: 1.2524195707255896e-15\n",
      "Fit enforced by user.\n",
      "Chi-Squared Test Statistic: 48.429563492063494, p-value: 9.696542051938611e-09\n",
      "Fit enforced by user.\n",
      "KS Test Statistic: 0.5310917142185438, p-value: 5.902010863192711e-19\n",
      "Fit enforced by user.\n",
      "KS Test Statistic: 0.4197295792121022, p-value: 1.0834030901005063e-11\n",
      "Fit enforced by user.\n",
      "Chi-Squared Test Statistic: 13.180395121331994, p-value: 0.06783319088209781\n",
      "Fit enforced by user.\n"
     ]
    }
   ],
   "source": [
    "# Fit univariate distributions\n",
    "if dist_fix == True:\n",
    "    dist_fix = -1\n",
    "else:\n",
    "    dist_fix = 0\n",
    "\n",
    "if sampling_type == 'univariate':\n",
    "    freq_params, og_frequency_samples, frequency_dist, freq_test = fit_frequency_distribution(result_df.groupby('Year', as_index=False)['Frequency'].mean()['Frequency'], frequency_dist, plot_RV, \"Frequency\", 100, status=dist_fix) #result_df.groupby('Year')['Frequency'].mean().reset_index()['Frequency']\n",
    "    intensity_params, og_intensity_samples, intensity_dist, int_test = fit_intensity_distribution(result_df['Intensity'][result_df['Frequency'] > 0], intensity_dist, plot_RV, \"Intensity\", 100, status=dist_fix)\n",
    "    duration_params, og_duration_samples, duration_dist, dur_test = fit_duration_distribution(result_df['Duration'][result_df['Frequency'] > 0], duration_dist, plot_RV, \"Duration\", 100, status=dist_fix)\n",
    "\n",
    "    dist_params = {\n",
    "        'intensity': (intensity_params, intensity_dist),\n",
    "        'duration': (duration_params, duration_dist),\n",
    "        'frequency': (freq_params, frequency_dist),\n",
    "    }\n",
    "\n",
    "    dists = pd.DataFrame({\n",
    "        'Model_Component': list(dist_params.keys())+['Forecast_Model'],\n",
    "        'Model_Fit': [intensity_dist,duration_dist,frequency_dist, forecast_model if (forecast_model == 'LSTM') else forecast_model + f'({forecast_params})'],\n",
    "        'Model_Evaluation_1': ['KS Test', 'Chi-Squared Test', 'Chi-Squared Test', 'MLSE' if (forecast_model == 'LSTM') else 'AIC'],\n",
    "        'Evaluation_Value_1': [int_test[0], dur_test[0], freq_test[0], LSTM_log_mse if (forecast_model == 'LSTM') else AIC],\n",
    "        'Model_Evaluation_2': ['p_val', 'p_val', 'p_val', 'N/A' if (forecast_model == 'LSTM') else 'BIC'],\n",
    "        'Evaluation_Value_2': [int_test[1], dur_test[1], freq_test[1], LSTM_log_mse if (forecast_model == 'LSTM') else BIC],\n",
    "    })\n",
    "    \n",
    "else:\n",
    "    freq_signal_params, freq_og_signal_samples, freq_signal_dist, freq_sig_test = fit_intensity_distribution(result_df.groupby('Year', as_index=False)['Signal'].mean()['Signal'], signal_dist, plot_RV, \"Signal\", 100, status=dist_fix) #result_df.groupby('Year')['Signal'].mean().reset_index()['Signal'][result_df['Signal'] > 0]\n",
    "    freq_params, og_frequency_samples, frequency_dist, freq_test = fit_frequency_distribution(result_df.groupby('Year', as_index=False)['Frequency'].mean()['Frequency'], frequency_dist, plot_RV, \"Frequency\", 100, status=dist_fix) #result_df.groupby('Year')['Frequency'].mean().reset_index()['Frequency']\n",
    "    signal_params, og_signal_samples, signal_dist, sig_test = fit_intensity_distribution(result_df['Signal'][result_df['Frequency'] > 0], signal_dist, plot_RV, \"Signal\", 100, status=dist_fix) #result_df.groupby('Year')['Signal'].mean().reset_index()['Signal'][result_df['Signal'] > 0]\n",
    "    intensity_params, og_intensity_samples, intensity_dist, int_test = fit_intensity_distribution(result_df['Intensity'][result_df['Frequency'] > 0], intensity_dist, plot_RV, \"Intensity\", 100, status=dist_fix)\n",
    "    duration_params, og_duration_samples, duration_dist, dur_test = fit_duration_distribution(result_df['Duration'][result_df['Frequency'] > 0], duration_dist, plot_RV, \"Duration\", 100, status=dist_fix)\n",
    "\n",
    "    dist_params = {\n",
    "        'signal_freq': (freq_signal_params, freq_signal_dist),\n",
    "        'signal': (signal_params, signal_dist),\n",
    "        'intensity': (intensity_params, intensity_dist),\n",
    "        'duration': (duration_params, duration_dist),\n",
    "        'frequency': (freq_params, frequency_dist),\n",
    "    }\n",
    "\n",
    "    dists = pd.DataFrame({\n",
    "        'Model_Component': list(dist_params.keys())+['Forecast_Model'],\n",
    "        'Model_Fit': [freq_signal_dist,signal_dist,intensity_dist,duration_dist,frequency_dist, forecast_model if (forecast_model == 'LSTM') else forecast_model + f'({forecast_params})'],\n",
    "        'Model_Evaluation_1': ['KS Test', 'KS Test', 'KS Test', 'Chi-Squared Test', 'Chi-Squared Test', 'Log_MSE' if (forecast_model == 'LSTM') else 'AIC'],\n",
    "        'Evaluation_Value_1': [freq_sig_test[0], sig_test[0], int_test[0], dur_test[0], freq_test[0], LSTM_log_mse if (forecast_model == 'LSTM') else AIC],\n",
    "        'Model_Evaluation_2': ['p_val', 'p_val', 'p_val', 'p_val', 'p_val', 'N/A' if (forecast_model == 'LSTM') else 'BIC'],\n",
    "        'Evaluation_Value_2': [freq_sig_test[1], sig_test[1], int_test[1], dur_test[1], freq_test[1], LSTM_log_mse if (forecast_model == 'LSTM') else BIC],\n",
    "    })\n",
    "\n",
    "if record_dists:\n",
    "    dists.to_csv(f\"{folder}/Dists_{run_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a439451f-64fa-4cef-8777-2642fb396e8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T22:10:34.896439Z",
     "iopub.status.busy": "2025-11-23T22:10:34.896285Z",
     "iopub.status.idle": "2025-11-23T22:10:34.899765Z",
     "shell.execute_reply": "2025-11-23T22:10:34.899237Z"
    },
    "papermill": {
     "duration": 0.011926,
     "end_time": "2025-11-23T22:10:34.900241",
     "exception": false,
     "start_time": "2025-11-23T22:10:34.888315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize forecasting and signal parameterization vector\n",
    "max_time = np.max(time)\n",
    "    \n",
    "# Create a 2D array for the 'year' column\n",
    "years = np.arange(1, steps + 1) + max_time  # Create list of years to forecast for\n",
    "\n",
    "# Repeat 'years' for each simulation (y times)\n",
    "year_column = np.tile(years, n_simulations)\n",
    "\n",
    "# Repeat simulation indices for each year (x times)\n",
    "sim_column = np.repeat(np.arange(n_simulations), steps)\n",
    "\n",
    "if forecast_model == 'LSTM':\n",
    "    signal_column = np.tile(fin_forecast, n_simulations)\n",
    "else:\n",
    "    # Flatten the forecasted_agg_ibc array for the 'signal' column\n",
    "    signal_column = fin_forecast.flatten()\n",
    "    \n",
    "# Create the DataFrame\n",
    "future_signal = pd.DataFrame({\n",
    "    'year': year_column,\n",
    "    'sim': sim_column,\n",
    "    'signal': signal_column\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb0337cb-5866-4949-9fed-823f1abaad39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T22:10:34.916913Z",
     "iopub.status.busy": "2025-11-23T22:10:34.916757Z",
     "iopub.status.idle": "2025-11-23T22:10:35.009832Z",
     "shell.execute_reply": "2025-11-23T22:10:35.009210Z"
    },
    "papermill": {
     "duration": 0.101092,
     "end_time": "2025-11-23T22:10:35.010359",
     "exception": false,
     "start_time": "2025-11-23T22:10:34.909267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stationary Signal Parameters Derived Successfully.\n"
     ]
    }
   ],
   "source": [
    "# Specify parameters for univariate fits based on sampling and nonstationarity type\n",
    "if sampling_type != 'univariate':\n",
    "    if nonstationary_type == 'KNN':\n",
    "        future_signal = KNN(future_signal, multivar_signal)\n",
    "        dist_params['signal'] = (signal_params, 'Expon')\n",
    "        dist_params['signal_freq'] = (freq_signal_params, 'Expon')\n",
    "        dist_params['intensity'] = (intensity_params, 'Expon')\n",
    "        dist_params['duration'] = (duration_params, 'Expon')\n",
    "        print(\"Nonstationary Signal Parameters Derived Successfully.\")\n",
    "    elif nonstationary_type == 'KNN_MLE':\n",
    "        future_signal = KNN_MLE(future_signal, multivar_signal)\n",
    "        dist_params['signal'] = (signal_params, 'Expon')\n",
    "        dist_params['signal_freq'] = (freq_signal_params, 'Expon')\n",
    "        dist_params['intensity'] = (intensity_params, 'Expon')\n",
    "        dist_params['duration'] = (duration_params, 'Expon')\n",
    "        print(\"Nonstationary Signal Parameters Derived Successfully.\")\n",
    "    elif nonstationary_type == 'scaled':\n",
    "        min_sig = min(np.min(result_df['Signal']), np.min(future_signal['signal']))\n",
    "        max_sig = max(np.max(result_df['Signal']), np.max(future_signal['signal']))\n",
    "        scaled = (future_signal['signal'] - min_sig) / (max_sig - min_sig) + 0.5\n",
    "        scaled = scaled*scale_factor \n",
    "        if signal_dist != 'Logspline':\n",
    "            future_signal['Scale_Sig'] = scaled*signal_params['scale']\n",
    "        if intensity_dist != 'Logspline':\n",
    "            future_signal['Scale_Int'] = scaled*intensity_params['scale']\n",
    "        if duration_dist != 'Logspline':    \n",
    "            future_signal['Scale_Dur'] = scaled*duration_params['scale']\n",
    "        if frequency_dist != 'Logspline':    \n",
    "            future_signal['Scale_Freq'] = scaled*freq_params['lambda']\n",
    "        print(\"Nonstationary Signal Parameters Derived Successfully.\")\n",
    "    else:\n",
    "        if signal_dist != 'Logspline':\n",
    "            future_signal['Scale_Sig'] = 1*signal_params['scale']\n",
    "        if intensity_dist != 'Logspline':\n",
    "            future_signal['Scale_Int'] = 1*intensity_params['scale']\n",
    "        if duration_dist != 'Logspline':\n",
    "            future_signal['Scale_Dur'] = 1*duration_params['scale']\n",
    "        if frequency_dist != 'Logspline':\n",
    "            future_signal['Scale_Freq'] = 1*freq_params['lambda']\n",
    "        print(\"Stationary Signal Parameters Derived Successfully.\")\n",
    "\n",
    "    if save_params:\n",
    "        future_signal.to_csv(f\"{folder}/Params_{run_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8162685-50f7-4e9e-b413-a22b4ed9c573",
   "metadata": {
    "papermill": {
     "duration": 0.007256,
     "end_time": "2025-11-23T22:10:35.024647",
     "exception": false,
     "start_time": "2025-11-23T22:10:35.017391",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Adapted Neyman-Scott Process storm generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0ce7074-ef64-47ca-8fa2-de21b7b9d350",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T22:10:35.040219Z",
     "iopub.status.busy": "2025-11-23T22:10:35.040033Z",
     "iopub.status.idle": "2025-11-23T22:13:20.307858Z",
     "shell.execute_reply": "2025-11-23T22:13:20.307108Z"
    },
    "papermill": {
     "duration": 165.276432,
     "end_time": "2025-11-23T22:13:20.308722",
     "exception": false,
     "start_time": "2025-11-23T22:10:35.032290",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation 100 completed out of 1000 in 16.62 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation 200 completed out of 1000 in 33.05 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation 300 completed out of 1000 in 49.47 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation 400 completed out of 1000 in 65.95 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation 500 completed out of 1000 in 82.70 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation 600 completed out of 1000 in 98.94 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation 700 completed out of 1000 in 115.55 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation 800 completed out of 1000 in 132.31 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation 900 completed out of 1000 in 148.65 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation 1000 completed out of 1000 in 164.76 seconds.\n",
      "1000 Simulations Complete. Run time: 164.76 seconds.\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "start_time = tm.time()  # Record the start time before the simulation begins\n",
    "\n",
    "if sampling_type == 'univariate':\n",
    "    all_sims, traj_parent = uni_adapted_NS_Process(n_simulations=n_simulations, \n",
    "                       years=years,\n",
    "                       forecasted_agg_ibc=forecasted_agg_ibc,\n",
    "                       reconstruct_forecast=reconstruct_forecast,\n",
    "                       summary_std=result_df,\n",
    "                       cluster_dict_std=cluster_dict_std,\n",
    "                       folder=folder,\n",
    "                       run_name=run_name,\n",
    "                       base_signal=base_signal,\n",
    "                       percentile_50=percentile_50,\n",
    "                       forecast_model=forecast_model,\n",
    "                       intensity_dist=intensity_dist,\n",
    "                       intensity_params=intensity_params,\n",
    "                       dur_dist=duration_dist,\n",
    "                       duration_params=duration_params,\n",
    "                       KNN_Type=nonstationary_type,\n",
    "                       steps = steps)\n",
    "    end_time = tm.time()  # Record the end time after the simulation ends\n",
    "    elapsed_time = end_time - start_time  # Calculate the elapsed time\n",
    "    print(f\"{n_simulations} Simulations completed in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "else:\n",
    "    all_data = []\n",
    "    for sim in range(n_simulations):\n",
    "        fut_sig = future_signal[future_signal['sim'] == sim].reset_index(drop=True)\n",
    "        if forecast_model == 'LSTM':\n",
    "            FS_parent = simulate_storm_frequencies(sim, years, fut_sig, result_df, dist_params, KNN_sampling=KNN_sampling, plot_pair=False, plot_RV=False)\n",
    "        else:\n",
    "            FS_parent = simulate_storm_frequencies(sim, years, fut_sig, result_df, dist_params, KNN_sampling=KNN_sampling, plot_pair=False, plot_RV=False)\n",
    "        parent = expand_rows_based_on_frequency(FS_parent, sim)\n",
    "        FIDS_parent = simulate_storm_statistics(sim, parent, result_df, fut_sig, dist_params, KNN_sampling=KNN_sampling, plot_pair=False, plot_RV=False)\n",
    "        traj_parent = storm_trajectories(FIDS_parent, bootstrap_curve, cluster_dict_std, plot=plot_traj)\n",
    "        all_data.append(traj_parent)\n",
    "        if (sim+1) % 100 == 0:\n",
    "            end_time = tm.time()  # Record the end time after the simulation ends\n",
    "            elapsed_time = end_time - start_time  # Calculate the elapsed time\n",
    "            print(f\"Simulation {sim+1} completed out of {n_simulations} in {elapsed_time:.2f} seconds.\")\n",
    "        \n",
    "    end_time = tm.time()  # Record the end time after the simulation ends\n",
    "    elapsed_time = end_time - start_time  # Calculate the elapsed time\n",
    "    print(f\"{n_simulations} Simulations Complete. Run time: {elapsed_time:.2f} seconds.\")\n",
    "    \n",
    "    # Concatenate all DataFrames in the list at once\n",
    "    all_sims = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Convert year and sim columns to integers\n",
    "    all_sims = all_sims.astype({'sim': 'int', 'year': 'int'})\n",
    "    all_sims = all_sims[['sim', 'year', 'storm_index', 'unique_storm_id', 'signal', 'Frequency', 'Intensity', 'Duration', 'storm_day', 'daily_flow']]  # Reorder columns\n",
    "    \n",
    "    # Sort the DataFrame by 'unique_storm_id'\n",
    "    all_sims = all_sims.sort_values(by='unique_storm_id')\n",
    "    # Save to CSV\n",
    "    all_sims.to_csv(f\"{folder}/All_Sims_{run_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f05c5d-d3ba-4a3e-a228-1e0d70a132bb",
   "metadata": {
    "papermill": {
     "duration": 0.008551,
     "end_time": "2025-11-23T22:13:20.326148",
     "exception": false,
     "start_time": "2025-11-23T22:13:20.317597",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Summary Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73919d6f-c067-48a5-8005-14b93d498dc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T22:13:20.343034Z",
     "iopub.status.busy": "2025-11-23T22:13:20.342816Z",
     "iopub.status.idle": "2025-11-23T22:13:20.347665Z",
     "shell.execute_reply": "2025-11-23T22:13:20.347125Z"
    },
    "papermill": {
     "duration": 0.014711,
     "end_time": "2025-11-23T22:13:20.348322",
     "exception": false,
     "start_time": "2025-11-23T22:13:20.333611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Done!\n"
     ]
    }
   ],
   "source": [
    "if plot_allsims:\n",
    "    if sampling_type == 'univariate':\n",
    "        uni_plot_violin(\n",
    "            data=all_sims, \n",
    "            group_by_columns=['year', 'sim'], \n",
    "            y_column='storm_max_intensity', \n",
    "            title='Maximum Annual Storm Intensities Over Time, All Simulations', \n",
    "            y_label='Max Annual Intensity', \n",
    "            percentile_95=percentile_95,  # Replace with your actual value\n",
    "            f_100=f_100  # Replace with your actual value\n",
    "        )\n",
    "        \n",
    "        # Duration over all sims\n",
    "        uni_plot_violin(\n",
    "            data=all_sims, \n",
    "            group_by_columns=['year', 'sim'], \n",
    "            y_column='storm_duration', \n",
    "            title='Average Annual Storm Duration Over Time, All Simulations', \n",
    "            y_label='Storm Duration (Days)'\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Frequency over all sims\n",
    "        uni_plot_violin(\n",
    "            data=all_sims, \n",
    "            group_by_columns=['year', 'sim'], \n",
    "            y_column='no_storms', \n",
    "            title='Annual Storm Frequencies Over Time, All Simulations', \n",
    "            y_label='Number of Storms'\n",
    "        )\n",
    "    else:\n",
    "        # Intensity over all sims\n",
    "        plot_violin(\n",
    "            data=all_sims, \n",
    "            group_by_columns=['year', 'sim'], \n",
    "            y_column='Intensity', \n",
    "            title='Maximum Annual Storm Intensities Over Time, All Simulations', \n",
    "            y_label='Max Annual Intensity', \n",
    "            percentile_95=percentile_95,  # Replace with your actual value\n",
    "            f_100=f_100  # Replace with your actual value\n",
    "        )\n",
    "        \n",
    "        # Duration over all sims\n",
    "        plot_violin(\n",
    "            data=all_sims, \n",
    "            group_by_columns=['year', 'sim'], \n",
    "            y_column='Duration', \n",
    "            title='Average Annual Storm Duration Over Time, All Simulations', \n",
    "            y_label='Storm Duration (Days)'\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Frequency over all sims\n",
    "        plot_violin(\n",
    "            data=all_sims, \n",
    "            group_by_columns=['year', 'sim'], \n",
    "            y_column='Frequency', \n",
    "            title='Annual Storm Frequencies Over Time, All Simulations', \n",
    "            y_label='Number of Storms'\n",
    "        )\n",
    "    \n",
    "if plot_onesim:\n",
    "    if sampling_type == 'univariate':\n",
    "        uni_plot_storm_intensities(traj_parent, percentile_50, base_signal, f_100)\n",
    "    else:\n",
    "        plot_storm_intensities(traj_parent, percentile_50, base_signal, f_100)\n",
    "\n",
    "print(\"All Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cad46d8-5aff-4c8f-b7e7-2fec54d37f46",
   "metadata": {
    "papermill": {
     "duration": 0.00801,
     "end_time": "2025-11-23T22:13:20.363844",
     "exception": false,
     "start_time": "2025-11-23T22:13:20.355834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 178.877163,
   "end_time": "2025-11-23T22:13:21.287961",
   "environment_variables": {},
   "exception": null,
   "input_path": "flood_simulator_main.ipynb",
   "output_path": "output_notebook.ipynb",
   "parameters": {
    "data_source": "ERA5_norm_vae",
    "end_date": "2009-12-31",
    "folder": "~/signal-extraction/results/ERA5_norm_vae",
    "run_name": "ERA5_norm_vae_KFold_ARIMA_default_3",
    "start_date": "1940-01-01",
    "steps": 15
   },
   "start_time": "2025-11-23T22:10:22.410798",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}